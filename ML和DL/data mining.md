# 数据挖掘

- [目录](#)
	- [基本统计描述](#基本统计描述)
	- [数据预处理](#数据预处理)
	- [挖掘频繁模式、关联和相关性](#挖掘频繁模式和关联和相关性)
	- [特征处理](#特征处理)
		- [初级篇](#初级篇)
		- [中级篇](#中级篇)
		- [高级篇](#高级篇)
	- [聚类](#聚类)
		- [DBSCAN](#DBSCAN)
		- [谱聚类](#谱聚类)
	- [离群点检测](#离群点检测)
	- [评分卡模型剖析](#评分卡模型剖析)
		- [使用IV和WOE目的](#使用IV和WOE目的)
		- [WOE](#WOE)
		- [IV](#IV)



## 基本统计描述
+ **度量数据中心趋势**
	+ 均值、加权平均、中位数、众数、中列数(最大值和最小值的平均值)

+ **度量数据散布**
	+ 极差：最大值和最小值之差
	+ n-分位数：将数据划分成n个大小相等的连贯集。
		+ 二分位数：Q2，即中位数
		+ 四分位数：将数据划分成4个大小相等的连贯集。第一个四分位数称为Q1(数据的前25%)，第二个四分位数称为Q2(数据的前50%)，第三个四分位数称为Q3(数据的前75%)。
		+ 四分位数极差IQR：IQR=Q3-Q1
	+ 方差、标准差：指出数据分布的散布程度，低标准差表明数据靠近均值。

+ **图形**	
	+ 盒图：也称箱线图，用来比较若干个可比较的数据集。
		+ 五数概括：最小值、Q1、Q2、Q3、最大值
			+ Q2：中位数由盒内的线标记；
			+ Q3和Q1：盒的端点一般在四分位数上，使得盒的长度是四分位数极差IQR；
			+ 最小值和最大值：盒外的两条线(称胡须)延伸到最小和最大观测值。当最小值和最大值超出四分位数的距离不到1.5 x IQR时，胡须扩展到它们，也就是说，胡须最大扩展到四分位数±1.5 x IQR的位置。
		+ 离群点：Q3之上或Q1之下至少1.5 x IQR处的值。
	+ 散点图：描述两个数值变量之间是否存在联系；
		+ 正相关：递增
		+ 负相关：递减
	+ 分位数图、分位数-分位数图、直方图

+ **相异性和相似性度量**
	+ 度量方法
		+ 非对称二元属性：对于仅有两个可能状态的标称属性，如果两个状态不同等重要，则称非对称二元属性；
			+ Jaccard系数
		+ 对称二元属性：对于仅有两个可能状态的标称属性，如果两个状态同等重要，则称对称二元属性；
		+ 曼哈顿距离：L1范数
		+ 欧几里得距离：L2范数
		+ 闵可夫斯基距离：Lp范数
		+ 上确界距离(切比雪夫距离)：L无穷范数；
		+ 对于稀疏数据，余弦度量和Tanimoto系数；
		+ pearson相关系数：衡量两个数据集是否在一条线上；用来描述两组线性的数据一同变化移动的趋势。
			+ pearson相关系数等于两个变量的协方差除于两个变量的标准差。
			+ **连续数据**，**正态分布**，**线性关系**，用pearson相关系数；上述任一条件不满足，就用spearman相关系数，不能用pearson相关系数；两个定序测量数据之间也用spearman相关系数，不能用pearson相关系数。
            + ```python
            def pearson(a,b):
                '''
                :description：计算皮尔逊系数
                :param a: list类型
                :param b: list类型
                :return: Double
                '''
                a_mean = np.mean(a)
                b_mean = np.mean(b)
                fenzi = np.sum([(a[i]-a_mean)*(b[i]-b_mean) for i in range(len(a))])
                fenmu0 = np.sqrt(np.sum([(a[i]-a_mean)*(a[i]-a_mean) for i in range(len(a))]))
                fenmu1 = np.sqrt(np.sum([(b[i]-b_mean)*(b[i]-b_mean) for i in range(len(b))]))
                try:
                    r = fenzi/(fenmu0*fenmu1)
                except ZeroDivisionError:
                    print("分母为0")
                return r
            ```
            
	+ 应用
		+ 聚类、离群点分析、最近邻分类等

## 数据预处理		
+ **数据清理**
	
	+ 缺失值处理
		+ 识别缺失值
			+ DataFrame.isnull()
			+ DataFrame.notnull()
			+ DataFrame.isna()
			+ DataFrame.notna()
			
		+ 处理缺失值
			+ 忽略元组：缺少类标号时,DataFrame.dropna(self, axis=0, how='any', thresh=None, subset=None, inplace=False)
			+ 人工填写缺失值：DataFrame.fillna(value=None, method=None, axis=None, inplace=False, limit=None)
				+ 使用一个全局常量填充，如：“Unknow”
				+ 使用属性的中心度量填充：中位数(倾斜数据而言)或均值
				+ 使用与给定元组属于同一类的所有样本的属性均值或中位数
				+ 使用最可能的值填充缺失值：决策树、贝叶斯推理
	
	+ 重复数据处理
		+ 样本重复：pandas.DataFrame(Series).drop_duplicates(self, subset=None, keep='first', inplace=False)
		
		+ 特征重复
			+ 通用
			```python
			def FeatureEquals(df):
			    dfEquals = pd.DataFrame([], columns=df.columns, index=df.columns)
				for i in df.columns:
				    for j in df.columns:
					    dfEquals.loc[i, j] = df.loc[:,i].equals(df.loc[:,j])
				return dfEquals
			
			```
			
			+ 数值型特征
			```python
			def drop_feature(data, way='pearson', assoRate=1.0):
			    '''
				此函数用于求取相似度大于assoRate的两列中的一个，主要目的是用于去除数值型特征的重复。
				data: 数据框，无默认
				assoRate: 相似度，默认为1
			    '''
				assoMat = data.corr(method = way)
				delCol = []
				length = len(assoMat)
				for i in range(length):
				    for j in range(i+1, length):
					    if assoMat.iloc[i, j] >= assoRate:
						    delCol.append(assoMat.columns[j])
				return delCol
			
			```
	
	+ 平滑处理
		+ np.log1p：log(x+1)，对偏度较大的数据进行转化，使其更加服从高斯分布；能避免复值问题——复值指一个自变量对应多个因变量。
		+ np.exmp1：exp(x)-1，np.log1p的逆运算。

	+ 噪声数据
		+ 分箱：将有序的值分布到箱中，并用周围的值来光滑有序数据。
			+ 用箱均值光滑
			+ 用箱中位数光滑
			+ 用箱边界光滑
		+ 回归
		+ 离群点分析
			+ 利用聚类来检测离群点
			
	+ 合并数据
		+ 数据堆叠：pandas.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True)
		+ 主键合并：pandas.merge(left, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False,suffixes=('_x', '_y'), copy=True, indicator=False)
		+ 重叠合并：pandas.DataFrame.combine_first(self, other)
		
	
		
+ **数据集成**
	+ 定义：合并来自多个数据存储的数据。
	+ 实体识别问题
	+ 冗余和相关分析
		+ 定义：一个属性可以由其他属性“导出”，称为冗余；利用相关分析来分析数据冗余，相关性越强，表明冗余度高。
		+ 卡方检验：标称数据
		+ 相关系数或协方差：数值属性
			+ 相关系数：Pearson系数
			+ 协方差：评估两个属性如何一起变化。
	+ 元组重复
	+ 数据值冲突的检测与处理

+ **数据归约**
	+ 维归约：把原数据变换或投影到较小的空间；
		+ 小波变换
		+ 主成分分析
		+ 属性子集选择(决策树中的做法)
	+ 数量归约：用较小的、替代的数据表示形式替换原数据；
		+ 参数方法：回归和对数-线性模型
		+ 非参数方法：直方图、聚类、抽样和数据立方体聚集
	+ 数据压缩：使用变换，以便得到原数据的归约或压缩表示；
		+ 有损压缩：只能近似重构原数据。
		+ 无损压缩：原数据能够从压缩后的数据重构，而不损失信息。
	
+ **数据变换和离散化**
	+ 光滑：去掉数据中的噪声；
		+ 分箱、回归和聚类
	+ 属性构造：可以由给定的属性构造新的属性并添加到属性集中；
	+ 聚集：对数据进行汇总或聚集；
	+ 规范化：把属性数据按比例缩放，使之落入一个特定的小区间；
		+ 标准差标准化：sklearn.preprocessing.StandardScaler
		+ 最小-最大规范化：也称离差标准化，sklearn.preprocessing.MinMaxScaler
		+ z分数规范化
		+ 小数定标规范化：通过移动属性值的小数点位置进行规范化；
	+ 离散化：数值属性的原始值用区间标签或概念标签替换；pandas.cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False)
		+ 通过分箱离散化
		+ 通过直方图分析离散化
		+ 通过聚类、决策树和相关分析离散化
	+ 由标称数据产生概念分层
	+ 哑变量处理：也叫独热编码，one-hot Encoding
		+ 将非数值类型变量进行独热编码：pandas.get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False)

+ **特征分箱的方法**
	+ **有监督**
		+ Best-KS
		+ ChiMerge
	+ **无监督**
		+ 等频：区间的边界值要经过选择,**使得每个区间包含大致相等的实例数量**。比如说 N=10 ,每个区间应该包含大约10%的实例。
		+ 等距：从最小值到最大值之间,均分为 N 等份, 这样, 如果 A,B 为最小最大值, 则每个区间的长度为 W=(B−A)/N , 则区间边界值为A+W,A+2W,….A+(N−1)W 。这里只考虑边界，每个等份里面的实例数量可能不等。
		+ 聚类

	
## 挖掘频繁模式和关联和相关性
> 基于水平数据格式挖掘频繁项集：Apriori算法和FP-growth算法；水平数据格式为{TID: itemset}，TID为事务标识符，itemset为事务TID中购买的商品。
> 基于垂直数据格式挖掘频繁项集：等价变换算法Eclat；垂直数据格式为{itemset: TID}。

+ **频繁项集挖掘方法**
	+ **Apriori算法**：通过限制候选产生发现频繁项集
		+ **连接步**：为找出Lk，通过将L(k-1)与自身连接产生候选k项集的集合Ck。
		+ **剪枝步**：Ck是Lk的超集，即Ck的成员可以是也可以不是频繁的，但所有的频繁k项集都包含在Ck中。为了压缩Ck，使用先验性质：任何非频繁的(k-1)项集都不是频繁k项集的子集，换句话说，如果一个候选k项集Ck的(k-1)项子集不在L(k-1)中，则可以删除该(k-1)项子集。这种子集测试可以使用所有频繁项集的散列树快速完成。
		+ 1. 在算法的第一次迭代中，每个项都是候选1项集的集合C1的成员。算法简单扫描所有的事务D，对每个项的出现次数计数，得到集合C1。
		+ 2. 根据给定的最小支持度计数，从集合C1中选出满足最小支持度计数的所有成员，构成频繁1项集的集合L1。
		+ 3. 使用连接(L1, L1)产生候选2项集的集合C2。C2由C(2,|L1|)个2项集组成。首先，删除集合C2中不满足先验条件的元素。然后，扫描事务D，累计C2中每个候选项集的支持计数。最后，从C2中选出满足给定的最小支持度计数的元素构成频繁2项集的集合L2。
		+ 4. 按照第3步的方法，使用连接(L2, L2)产生候选3项集的集合C3。首先，删除集合C3中不满足先验条件的元素。然后，扫描事务D，累计C3中每个候选项集的支持计数。最后，从C3中选出满足给定的最小支持度计数的元素构成频繁3项集的集合L3。
		+ 5. 依次迭代下去，直至集合C为空停止。
	
		+ **提高Apriori算法的效率**
			+ 基于散列的技术
			+ 事务压缩
			+ 划分
			+ 抽样
			+ 动态项集计数
	
	+ **挖掘频繁项集的模式增长法(FP-growth)**
		+ FP树的构建过程
			+ 1. 在算法的第一次迭代中，每个项都是候选1项集的集合C1的成员。算法简单扫描所有的事务D，对每个项的出现次数计数，得到集合C1，并根据支持度计数递减排序，得到L。
			+ 2. 创建树的根节点，用“null”标记。扫描事务D中每一个事务，然后按照L中的次序对事务中每一项item排序，并按照该顺序构建一条路径，并标记结点为对应的item以及计数值加1，依次迭代事务，假如已有分支，则共享分支并计数值加1，否则在根结点后创建新的分支。
			+ 3. 最后，创建一个项item头表，使每项通过一个结点链指向它在树中的位置。
		+ FP树的挖掘过程
			+ 由长度为1的频繁模式（初始后缀模式）开始，构造它的**条件模式基**（一个“子数据库”，由FP树中与该后缀模式一起出现的前缀路径集组成）。然后，构造它的条件FP树，并递归的在该树上进行挖掘。模式增长通过后缀模式与条件FP树产生的频繁模式连接实现。
		+ 优势
			+ 1. 相比Apriori算法，显著降低搜索开销。
			+ 2. 使用最不频繁的项作后缀，提供较好的选择性。
		
		
	+ **使用垂直数据格式挖掘频繁项集**
		+ 1. 对TID集做交运算，根据先验条件以及最小支持度计数得到频繁2项集L2；
		+ 2. 对频繁2项集的TID集做交运算，重复步骤1，直至不能再找到频繁项集或候选项集。

	+ **挖掘闭模式和极大模式**：闭频繁项集可以减少频繁模式挖掘所产生的模式数量，而且保持关于频繁项集的集合的完整信息。
		+ 项合并
		+ 子项集剪枝
		+ 项跳过

	+ **模式评估方法**
		+ 支持度—置信度：属于强关联规则。支持度表示同时购买的比例；置信度表示购买A产品的用户中购买B产品的比例。
			+ 该规则具有一定的欺骗性，它不会度量A和B之间相关和蕴含的实际强度。
		+ 利用相关分析扩充关联分析
			+ 使用**提升度**的相关分析：P(A U B)/(P(A)*P(B))
			
		+ 利用**卡方检验**进行相关分析

	+ **模式评估度量比较**
		+ 全置信度：min{P(A|B), P(B|A)}
		+ 最大置信度：max{P(A|B), P(B|A)}
		+ Kulczynski：0.5*(P(A|B)+P(B|A))
		+ 余弦：sqrt(P(A|B)*P(B|A))
		+ 不平衡比IR：(|A - B|)/(A+B-(A U B))
		
## 特征处理
> 单个原始特征(或变量)通常属于以下几类之一：
>    * 连续(continuous)特征
>    * 无序类别(categorical)特征
>    * 有序类别(ordinal)特征

### 初级篇
+ **连续特征**
	+  除了归一化（去中心，方差归一），不用做太多特殊处理，可以直接把连续特征扔到模型里使用。
	
+ **无序特征**
	+ 可以使用One-hot（也叫One-of-k）的方法把每个无序特征转化为一个数值向量。比如一个无序特征color有三种取值：red，green，blue。那么可以用一个长度为3的向量来表示它，向量中的各个值分别对应于red -> [1,0,0]；green -> [0,1,0]；blue -> [0,0,1]。前面说的表达方式里有一个维度是可以省略的。既然我们知道color一定是取3个值中的一个，那么我们知道向量的前两个元素值，就能推断第3个值是多少。所以，其实用下面的方式就可以表达到底是哪种颜色：red -> [1,0]；green -> [0,1]；blue -> [0,0]。这样表达的好处是少用了一个维度，降低了转化后特征之间的相关性。但在实际问题中特征基本都或多或少会有些缺失。使用第一种表达方式就可以用全0的向量来表示值缺失，而第二种表达方式是没法表达缺失的。

+ **有序特征**
> 有些特征虽然也像无序特征那样只取限定的几个值，但是这些值之间有顺序的含义。例如一个人的状态status有三种取值：bad, normal, good，显然bad < normal < good。
> 当然，对有序特征最简单的处理方式是忽略其中的顺序关系，把它看成无序的，这样我们就可以使用处理无序特征的方式来处理它。在实际问题中，这种处理方式其实用的很多。

> 当然有些问题里有序可能会很重要，这时候就不应该把其中的顺序关系丢掉。一般的表达方式如下：

| status取值 | 向量表示 |
| :------: | :------: |
| bad | (1,0,0) |
| normal | (1,1,0) |
| good | (1,1,1) |

> 上面这种表达方式很巧妙地利用递进表达了值之间的顺序关系。


### 中级篇
> 最容易让人掉以轻心的，往往就是大家觉得最简单的事。在特征处理中，最容易让刚入门同学忽略的，是对连续特征的处理方式。

> 以线性分类器Linear Regression (LinearReg)为例，它是通过特征的线性加权来预测因变量y：y = w<sub>T</sub>x。但大部分实际情况下，y与x都不会是这么简单的线性关系，甚至连单调关系都不会有。举个只有一个特征的例子，如果y与x的实际关系如下图：

![y与x的实际关系](https://i.loli.net/2019/04/25/5cc1baf538605.jpg)

> 那么直接把x扔进LinearReg模型是怎么也得不到好结果的。很多人会想着既然线性分类器搞不定，那就直接找个非线性的好了，比如高斯核的SVM。我们确实可以通过这种简单换算法的方式解决这个简单的问题。但对于很多实际问题（如广告点击率预测），往往特征非常多，这时候时间约束通常不允许我们使用很复杂的非线性分类器。这也是为什么算法发展这么多年，广告点击率预测最常用的方法还是Logistic Regression (LogisticReg)。
> 对于上面这个问题，有没有什么办法使得LinearReg也能处理得不错？**当然是有，就是对原始特征x做转化，把原来的非线性关系转化为线性关系。**

+ 方法一：离散化
	+ 最常用的转化方式是对x做离散化(discretization)，也就是把原来的值分段，转化成一个取值为0或1的向量。原始值落在某个段里，向量中此段对应的元素就为1，否则为0。**离散化的目标是y与转化后向量里的每个元素都保持比较好的线性关系**。
	+ 比如取离散点{0.5,1.5,2.5}，通过判断x属于(−∞,0.5)，[0.5,1.5)，[1.5,2.5)，[2.5,+∞)中哪段来把它离散化为4维的向量。下面是一些例子的离散结果：

	| 原始值x | 离散化后的值 |
	| :------: | :------: |
	| 0.1 | (1,0,0,0) |
	| 1.3 | (0,1,0,0) |
	| 3.2 | (0,0,1,0) |
	| 5.8 | (0,0,0,1) |

	+ 离散化方法的关键是怎么确定分段中的离散点。下面是常用的选取离散点的方法：
		+ **等距离离散**：顾名思义，就是离散点选取等距点。我们上面对x取离散点{0.5,1.5,2.5}就是一种等距离散。
		
		+ **等样本点离散**：选取的离散点保证落在每段里的样本点数量大致相同。
		
		+ **画图观察趋势**：以x为横坐标，y为纵坐标，画图，看曲线的趋势和拐点。通过观察下面的图我们发现可以利用3条直线（红色直线）来逐段近似原来的曲线。把离散点设为两条直线相交的各个点，我们就可以把x离散化为长度为3的向量。这种离散化为0/1向量的方法有个问题，它在离散时不会考虑到具体的x到离散边界的距离。比如等距离散中取离散点为{0.5,1.5,2.5}，那么1.499，1.501和2.49分别会离散为(0, 1, 0, 0)，(0, 0, 1, 0)和(0, 0, 1, 0)。1.499和1.501很接近，可是就因为这种强制分段的离散导致它们离散的结果差距很大。针对上面这种硬离散的一种改进就是使用**软离散**，也就是在离散时考虑到x与附近离散点的距离，离散出来的向量元素值可以是0/1之外的其他值。
		
![画图观察趋势](https://i.loli.net/2019/04/25/5cc1bdace800b.jpg)	
		
+ 方法二：函数变换
> **函数变换**直接把原来的特征通过非线性函数做变换，然后把原来的特征，以及变换后的特征一起加入模型进行训练。常用的变换函数见下表，不过其实你可以尝试任何函数。 

| 常用非线性函数f(x) | x的取值范围 |
| :------: | :------: |
| x<sub>a</sub>;a∈(-∞,+∞) | (-∞,+∞) |
| log(x) | (0,+∞) |
| log(x/(1-x)) | (0,1) |

> 这个方法操作起来很简单，但记得对新加入的特征做归一化。对于我们前面的问题，只要把x2，x3也作为特征加入即可，因为实际上y就是x的一个三次多项式。

### 高级篇

+ **笛卡尔乘积**

> 我们可以使用笛卡尔乘积的方式来组合2个或更多个特征。比如有两个类别特征color和light，它们分别可以取值为red，green，blue和on, off。这两个特征各自可以离散化为3维和2维的向量。对它们做笛卡尔乘积转化，就可以组合出长度为6的特征，它们分别对应着原始值对(red, on)，(red, off)，(green, on)，(green, off)，(blue, on)，(blue, off)。
> 对于3个特征的笛卡尔乘积组合，可以表达为立方的形式。更多特征的组合依次类推。这个方法也可以直接用于连续特征与类别特征之间的组合，只要把连续特征看成是1维的类别特征就好了，这时候组合后特征对应的值就不是0/1了，而是连续特征的取值。

+ **离散化续篇**

> 在上节中我已经介绍了一些常用的离散化单个连续特征的方法，其中一个是**画图观察趋势**。画图观察趋势的好处是直观、可解释性强，坏处是很麻烦。当要离散化的特征很多时，这种方法可操作性较差。

> 机器学习中有个很好解释，速度也不错的模型——决策树模型。大白话说决策树模型就是一大堆的if else。它天生就可以对连续特征分段，所以把它用于离散化连续特征合情合理。我称这种方法为决策树离散化方法。例如Gmail在对信件做重要性排序时就使用了决策树离散化方法2。

> 决策树离散化方法通常也是每次离散化一个连续特征，做法如下：单独用此特征和目标值y训练一个决策树模型，然后把训练获得的模型内的特征分割点作为离散化的离散点。

> 这种方法当然也可以同时离散化多个连续特征，但是操作起来就更复杂了，实际用的不多。

+ **核方法**
> 核方法经常作为线性模型的一种推广出现。

## 聚类

### DBSCAN
> 一种基于高密度连通区域的基于密度的聚类

+ **簇过程**
	+ 1. 给定数据集中所有对象都标记为"unvisited"。DBSCAN随机地选择一个未访问对象p，标记p为“visited”，并检查p的e-邻域是否至少包含MinPts对象，如果不是，标记p为**噪声点**。否则为p(p称为核心对象)创建一个新的簇C，并且把p的e-邻域中的所有对象都放到候选集合N中。
	+ 2. DBSCAN迭代地把N中不属于其他簇的对象添加到C中，在此过程中，对于N中标记为“unvisited”的对象p'，DBSCAN把它标记为"visited"，并且检查它的e-邻域。如果p'的e-邻域至少有MinPts个对象，则p'的e-邻域中的对象都被添加到N中。DBSCAN继续添加对象到C，直到C不能再扩展，即直到N为空，此时，簇C完全生成，被输出。
	+ 3. DBSCAN从剩下的对象中随机选择一个未访问的对象，聚类过程继续，直到所有对象都被访问。

### 谱聚类
+ **Ng**-**Jordan**-**Weiss算法**
	+ 1. 使用距离度量计算**相似矩阵W**；
	+ 2. 定义一个对角矩阵D，其中D<sub>ii</sub>是W第i行之和，使用相似矩阵W，导出矩阵A=D<sup>-1/2</sup>WD<sup>-1/2</sup>
	+ 3. 找出A的前k个特征向量，k应该比原数据空间的维度小得多；
	+ 4. 使用前k个最大特征值的特征向量，把原数据投影到由前k个特征向量定义的新空间，并运行聚类算法找出k个簇。
	+ 5. 根据变换后的点被分配到第4步得到的簇，把原数据点分配到这些簇。

## 离群点检测
> 又称异常检测，找出其行为很不同于预期对象的过程，这种对象称为离群点或异常。
> 离群点不同于噪声数据，噪声是被观测变量的随机误差或方差。

+ **离群点的类型**
	+ 全局离群点
	+ 情境离群点：关于对象的特定情境，它显著地偏离其他对象。
	+ 集体离群点：一些对象显著地偏离整个数据对象。
	
+ **离群点检测方法**
	+ 统计学方法
		+ 参数方法：假定正常数据对象服从某种参数分布。
			+ 基于正态分布的一元离群点检测：仅涉及一个属性或变量的数据为一元数据。使用最大似然检测一元数据，并把低概率的点识别为离群点。
				+ 3\theta原则——只适用于正态分布
				```python
				def outRange(Ser1):
				    boolInd = (Ser1.mean()-3*Ser1.std() > Ser1) | (Ser1.mean()+3*Ser1.var() < Ser1) # 找出Series中所有离群点
					index = np.arange(Ser1.shape[0])[boolInd]
					outrange = Ser1.iloc[index]
					
					return outrange
				
				```
				+ Grubb检验（最大标准残差检验）
				
				+ 箱线图分析
				```python
				def boxOutRange(Ser):
				   '''
				   Ser: 进行异常值分析的DataFrame的某一列
				   '''
				   Low = Ser.quantile(0.25)-1.5*(Ser.quantile(0.75)-Ser.quantile(0.25)) 
				   Up = Ser.quantile(0.75)+1.5*(Ser.quantile(0.75)-Ser.quantile(0.25))
				   index = (Ser<Low) | (Ser>Up)
				   outlier = Ser.loc[index]
				   
				   return outlier
				```
			+ 多元离群点检测：把多元离群点检测任务转换成一元离散点检测问题。使用马哈拉诺比斯距离检测多元离群点。或使用卡方统计量的多元离群点检测。
			+ 使用混合参数分布检测多元离群点：使用EM算法学习参数
			+ 使用多个簇检测多元离群点
			
		+ 非参数方法：没有假定先验
			+ 使用直方图检测离群点
			+ 使用核密度估计来估计数据的概率密度分布
			
	+ 基于邻近性的方法
		+ 基于距离的离群点检测和嵌套循环方法
		+ 基于距离的离群点检测的基于网格的方法
		+ 基于密度的离群点检测
	
	+ 基于聚类的方法
		+ 把离群点检测为不属于任何簇的对象
		+ 使用最近簇的距离的基于聚类的离群点检测
		+ 通过基于聚类的离群点检测进行入侵检测
		+ 检测小簇中的离群点
		
	+ 基于分类的方法
		+ 使用一类模型检测为离群点：构建一个仅描述正常类的分类器
		+ 通过半监督学习检测离群点：使用基于聚类的方法进行聚类，然后利用已知类别分析离群点。
		
	+ 挖掘情境离群点和集体离群点
		+ 把情境离群点检测转换为传统的离群点检测
		
	+ 高维数据中的离群点检测
		+ 扩充的离群点检测：使用传统的检测方法，修正其它度量方式。
		+ 发现子空间中的离群点
		+ 高维离群点建模
			+ 基于角的离群点
			
			
## 	评分卡模型剖析
[数据挖掘模型中的IV和WOE](https://blog.csdn.net/kevin7658/article/details/50780391)

> 信用评分卡模型最常见的模型是Logistic回归，它是一种成熟的预测方法，尤其在信用风险评估以及金融风险控制领域得到了广泛的使用。**其原理是将模型变量WOE编码方式离散化之后运用Logistic回归模型进行的一种二分类变量的广义线性模型**。
> 由于制作评分卡的某些需要，通常会在建立评分模型时将自变量做离散化处理（等宽切割，等高切割，或利用决策树切割），但是模型本身没有办法很好地直接接受分类自变量的输入，因此需要对自变量进行再次处理。比较常规的做法有两种：1、**做dummy变量**，2、**做基于目标的变量编码**。
> 做dummy变量：假定自变量m有3种取值：m1、m2和m3，则可以构造2个dummy变量M1和M2：m1 == M1(1)M2(0)；m2 == M1(0)M2(1)；m3 == M1(0)M2(0)。dummy变量也存在一些缺点，例如无法对自变量的每一个取值计算信用分，并且回归模型筛选变量时可能出现某个自变量被部分地舍弃的情况。
> 做基于目标的变量编码：在信用评分中比较常见的是WOE(Weight of Evidence)编码。

### 使用IV和WOE目的
+ **提升模型的预测效果**
+ **提高模型的可理解性**

### WOE
+ **证据权重**：Weight of Evidence，表示自变量取某个值对违约比例的影响。要对一个变量进行WOE编码，首先需要把这个变量进行**分组处理**(离散化、分箱等)。
+ **公式**：WOE<sub>i</sub> = ln(P<sub>y<sub>i</sub></sub>/P<sub>n<sub>i</sub></sub>)，其中P<sub>y<sub>i</sub></sub>是第i个分组中，预测值为1的样本占所有为1的样本的比例；P<sub>n<sub>i</sub></sub>是第i个分组中，预测值为0的样本占所有为0的样本的比例。

+ **优势**
	+ WOE能够反映自变量的贡献情况
		+ 自变量内部WOE值的变异（波动）情况，结合模型拟合出的系数，构造出各个自变量的贡献率及相对重要性。一般地，系数越大，WOE的方差越大，则自变量的贡献率越大。
	+ 标准化功能
		+ WOE编码后，自变量其实具备了某种标准化的性质，即自变量内部的各个取值之间可以直接进行比较（WOE之间的比较），而不同自变量之间的各种取值也可以通过WOE进行直接的比较。
	+ 对异常值不敏感
		+ 很多极值变量通过WOE可以变为非异常值，很多频次较少的变量也可以通过WOE转换进行合并。
	+ 对于评分卡模型，通过WOE转化，极大地提高了数据的可理解性。
		+ WOE描述了变量当前分组对判断个体是否会对响应（或者说属于哪个类）所起到的影响方向和大小，<br>当WOE为正时，变量当前取值对判断个体是否会对响应起到的正向的影响；反之。</br>
	+ WOE与违约概率具有某种线性关系
		+ 从而通过这种WOE编码可以发现自变量与目标变量之间的非线性关系（例如U型或者倒U型关系）。提升预测效果
	+ WOE变量出现负值情况
		+ 在此基础上，我们可以预料到模型拟合出来的自变量系数应该都是正数，如果结果中出现了负数，应当考虑是否是来自自变量多重共线性的影响。
+ 做回归分析时，生成WOE变量为什么一定要是线性的？
	+ 因为这样可以对于模型预测更好，并且更稳定。
+ 有些数据的特性本身就不是连续的，所以在用WOE分段的时候woe值就会出现正负交错的现象，我觉得这还是要结合现实逻辑，如果逻辑能够解释的通的，那么这样做线性回归也是可以的，如果现实逻辑解释不了的就要考虑样本偏差和数据质量的问题了。

			
```python
def total_response(data,label):
    value_count = data[label].value_counts()
    return value_count
def woe(data,feature,label,label_value):
    '''
    data：传入需要做woe的特征和标签两列数据
    feature：特征名
    label：标签名
    label_value:接收字典，key为1，0，表示为响应和未响应，value为对应的值
    '''
    import pandas as pd 
    import numpy as np
    idx = pd.IndexSlice  # 创建一个对象以更轻松地执行多索引切片
    
    data['woe'] = 1
    groups = data.groupby([label,feature]).count().sort_index()
    
    ## 使用上述函数
    value_counts = total_response(data,label)

    resp_col = label_value[1]
    not_resp_col = label_value[0]
    resp = groups.loc[idx[resp_col,:]]/value_counts[resp_col]
    not_resp = groups.loc[idx[not_resp_col,:]]/value_counts[not_resp_col]
    Woe = np.log(resp/not_resp).fillna(0)
    return Woe

```
----
	
### IV
+ **信息值**：Information Value，用来衡量自变量的预测能力，并根据这些量化指标的大小，来确定哪些变量进入模型。类似的指标还有信息增益、基尼系数等等。<br>IV值考虑了分组中样本占整体样本的比例，相当于WOE的加权求和。</br>
+ **公式**：IV<sub>i</sub> = (P(y<sub>i</sub>)-P(n<sub>i</sub>))× WOE<sub>i</sub> = (P(y<sub>i</sub>)-P(n<sub>i</sub>))×ln(P<sub>y<sub>i</sub></sub>/P<sub>n<sub>i</sub></sub>)，其中P<sub>y<sub>i</sub></sub>是第i个分组中，预测值为1的样本占所有为1的样本的比例；P<sub>n<sub>i</sub></sub>是第i个分组中，预测值为0的样本占所有为0的样本的比例。<br>IV = sum(IV<sub>i</sub>)</br>
+ **作用**：选择变量
	+ 非负指标
	+ 高IV表示该特征和目标变量的关联度高
	+ 目标变量只能是二分类
	+ 过高的IV,可能有潜在的风险
	+ 特征分箱越细,IV越高
	+ 常用的阈值有: < =0.02: 没有预测性,不可用；0.02 to 0.1: 弱预测性；0.1 to 0.2: 有一定的预测性；0.2+: 高预测性。
	

		
			
			
			
			
			
			
			
			
			
			
		



	