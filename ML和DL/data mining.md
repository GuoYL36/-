# 数据挖掘

- [目录](#)
	- [基本统计描述](#基本统计描述)
	- [数据预处理](#数据预处理)
	- [挖掘频繁模式、关联和相关性](#挖掘频繁模式和关联和相关性)
	- [特征处理](#特征处理)
		- [初级篇](#初级篇)
		- [中级篇](#中级篇)
		- [高级篇](#高级篇)
	- [聚类](#聚类)
		- [DBSCAN](#DBSCAN)
		- [谱聚类](#谱聚类)
	- [离群点检测](#离群点检测)


## 基本统计描述
+ **度量数据中心趋势**
	+ 均值、加权平均、中位数、众数、中列数(最大值和最小值的平均值)

+ **度量数据散布**
	+ 极差：最大值和最小值之差
	+ n-分位数：将数据划分成n个大小相等的连贯集。
		+ 二分位数：Q2，即中位数
		+ 四分位数：将数据划分成4个大小相等的连贯集。第一个四分位数称为Q1(数据的前25%)，第二个四分位数称为Q2(数据的前50%)，第三个四分位数称为Q3(数据的前75%)。
		+ 四分位数极差IQR：IQR=Q3-Q1
	+ 方差、标准差：指出数据分布的散布程度，低标准差表明数据靠近均值。

+ **图形**	
	+ 盒图：用来比较若干个可比较的数据集。
		+ 五数概括：最小值、Q1、Q2、Q3、最大值
			+ Q2：中位数由盒内的线标记；
			+ Q3和Q1：盒的端点一般在四分位数上，使得盒的长度是四分位数极差IQR；
			+ 最小值和最大值：盒外的两条线(称胡须)延伸到最小和最大观测值。当最小值和最大值超出四分位数的距离不到1.5 x IQR时，胡须扩展到它们，也就是说，胡须最大扩展到四分位数±1.5 x IQR的位置。
		+ 离群点：Q3之上或Q1之下至少1.5 x IQR处的值。
	+ 散点图：描述两个数值变量之间是否存在联系；
		+ 正相关：递增
		+ 负相关：递减
	+ 分位数图、分位数-分位数图、直方图

+ **相异性和相似性度量**
	+ 度量方法
		+ 非对称二元属性：对于仅有两个可能状态的标称属性，如果两个状态不同等重要，则称非对称二元属性；
			+ Jaccard系数
		+ 对称二元属性：对于仅有两个可能状态的标称属性，如果两个状态同等重要，则称对称二元属性；
		+ 曼哈顿距离：L1范数
		+ 欧几里得距离：L2范数
		+ 闵可夫斯基距离：Lp范数
		+ 上确界距离(切比雪夫距离)：L无穷范数；
		+ 对于稀疏数据，余弦度量和Tanimoto系数；
		+ pearson相关系数：衡量两个数据集是否在一条线上；用来描述两组线性的数据一同变化移动的趋势。
			+ pearson相关系数等于两个变量的协方差除于两个变量的标准差。
			+ 连续数据，正态分布，线性关系，用pearson相关系数；上述任一条件不满足，就用spearman相关系数，不能用pearson相关系数；两个定序测量数据之间也用spearman相关系数，不能用pearson相关系数。
	
	+ 应用
		+ 聚类、离群点分析、最近邻分类等

## 数据预处理		
+ **数据清理**
	+ 缺失值处理：
		+ 忽略元组：缺少类标号时
		+ 人工填写缺失值
		+ 使用一个全局常量填充，如：“Unknow”
		+ 使用属性的中心度量填充：中位数(倾斜数据而言)或均值
		+ 使用与给定元组属于同一类的所有样本的属性均值或中位数
		+ 使用最可能的值填充缺失值：决策树、贝叶斯推理

	+ 噪声数据
		+ 分箱：将有序的值分布到箱中，并用周围的值来光滑有序数据。
			+ 用箱均值光滑
			+ 用箱中位数光滑
			+ 用箱边界光滑
		+ 回归
		+ 离群点分析
			+ 利用聚类来检测离群点
		
+ **数据集成**
	+ 定义：合并来自多个数据存储的数据。
	+ 实体识别问题
	+ 冗余和相关分析
		+ 定义：一个属性可以由其他属性“导出”，称为冗余；利用相关分析来分析数据冗余，相关性越强，表明冗余度高。
		+ 卡方检验：标称数据
		+ 相关系数或协方差：数值属性
			+ 相关系数：Pearson系数
			+ 协方差：评估两个属性如何一起变化。
	+ 元组重复
	+ 数据值冲突的检测与处理

+ **数据归约**
	+ 维归约：把原数据变换或投影到较小的空间；
		+ 小波变换
		+ 主成分分析
		+ 属性子集选择(决策树中的做法)
	+ 数量归约：用较小的、替代的数据表示形式替换原数据；
		+ 参数方法：回归和对数-线性模型
		+ 非参数方法：直方图、聚类、抽样和数据立方体聚集
	+ 数据压缩：使用变换，以便得到原数据的归约或压缩表示；
		+ 有损压缩：只能近似重构原数据。
		+ 无损压缩：原数据能够从压缩后的数据重构，而不损失信息。
	
+ **数据变换和离散化**
	+ 光滑：去掉数据中的噪声；
		+ 分箱、回归和聚类
	+ 属性构造：可以由给定的属性构造新的属性并添加到属性集中；
	+ 聚集：对数据进行汇总或聚集；
	+ 规范化：把属性数据按比例缩放，使之落入一个特定的小区间；
		+ 最小-最大规范化
		+ z分数规范化
		+ 小数定标规范化：通过移动属性值的小数点位置进行规范化；
	+ 离散化：数值属性的原始值用区间标签或概念标签替换；
		+ 通过分箱离散化
		+ 通过直方图分析离散化
		+ 通过聚类、决策树和相关分析离散化
	+ 由标称数据产生概念分层



	
## 挖掘频繁模式和关联和相关性
> 基于水平数据格式挖掘频繁项集：Apriori算法和FP-growth算法；水平数据格式为{TID: itemset}，TID为事务标识符，itemset为事务TID中购买的商品。
> 基于垂直数据格式挖掘频繁项集：等价变换算法Eclat；垂直数据格式为{itemset: TID}。

+ **频繁项集挖掘方法**
	+ **Apriori算法**：通过限制候选产生发现频繁项集
		+ **连接步**：为找出Lk，通过将L(k-1)与自身连接产生候选k项集的集合Ck。
		+ **剪枝步**：Ck是Lk的超集，即Ck的成员可以是也可以不是频繁的，但所有的频繁k项集都包含在Ck中。为了压缩Ck，使用先验性质：任何非频繁的(k-1)项集都不是频繁k项集的子集，换句话说，如果一个候选k项集Ck的(k-1)项子集不在L(k-1)中，则可以删除该(k-1)项子集。这种子集测试可以使用所有频繁项集的散列树快速完成。
		+ 1. 在算法的第一次迭代中，每个项都是候选1项集的集合C1的成员。算法简单扫描所有的事务D，对每个项的出现次数计数，得到集合C1。
		+ 2. 根据给定的最小支持度计数，从集合C1中选出满足最小支持度计数的所有成员，构成频繁1项集的集合L1。
		+ 3. 使用连接(L1, L1)产生候选2项集的集合C2。C2由C(2,|L1|)个2项集组成。首先，删除集合C2中不满足先验条件的元素。然后，扫描事务D，累计C2中每个候选项集的支持计数。最后，从C2中选出满足给定的最小支持度计数的元素构成频繁2项集的集合L2。
		+ 4. 按照第3步的方法，使用连接(L2, L2)产生候选3项集的集合C3。首先，删除集合C3中不满足先验条件的元素。然后，扫描事务D，累计C3中每个候选项集的支持计数。最后，从C3中选出满足给定的最小支持度计数的元素构成频繁3项集的集合L3。
		+ 5. 依次迭代下去，直至集合C为空停止。
	
		+ **提高Apriori算法的效率**
			+ 基于散列的技术
			+ 事务压缩
			+ 划分
			+ 抽样
			+ 动态项集计数
	
	+ **挖掘频繁项集的模式增长法(FP-growth)**
		+ FP树的构建过程
			+ 1. 在算法的第一次迭代中，每个项都是候选1项集的集合C1的成员。算法简单扫描所有的事务D，对每个项的出现次数计数，得到集合C1，并根据支持度计数递减排序，得到L。
			+ 2. 创建树的根节点，用“null”标记。扫描事务D中每一个事务，然后按照L中的次序对事务中每一项item排序，并按照该顺序构建一条路径，并标记结点为对应的item以及计数值加1，依次迭代事务，假如已有分支，则共享分支并计数值加1，否则在根结点后创建新的分支。
			+ 3. 最后，创建一个项item头表，使每项通过一个结点链指向它在树中的位置。
		+ FP树的挖掘过程
			+ 由长度为1的频繁模式（初始后缀模式）开始，构造它的**条件模式基**（一个“子数据库”，由FP树中与该后缀模式一起出现的前缀路径集组成）。然后，构造它的条件FP树，并递归的在该树上进行挖掘。模式增长通过后缀模式与条件FP树产生的频繁模式连接实现。
		+ 优势
			+ 1. 相比Apriori算法，显著降低搜索开销。
			+ 2. 使用最不频繁的项作后缀，提供较好的选择性。
		
		
	+ **使用垂直数据格式挖掘频繁项集**
		+ 1. 对TID集做交运算，根据先验条件以及最小支持度计数得到频繁2项集L2；
		+ 2. 对频繁2项集的TID集做交运算，重复步骤1，直至不能再找到频繁项集或候选项集。

	+ **挖掘闭模式和极大模式**：闭频繁项集可以减少频繁模式挖掘所产生的模式数量，而且保持关于频繁项集的集合的完整信息。
		+ 项合并
		+ 子项集剪枝
		+ 项跳过

	+ **模式评估方法**
		+ 支持度—置信度：属于强关联规则。支持度表示同时购买的比例；置信度表示购买A产品的用户中购买B产品的比例。
			+ 该规则具有一定的欺骗性，它不会度量A和B之间相关和蕴含的实际强度。
		+ 利用相关分析扩充关联分析
			+ 使用**提升度**的相关分析：P(A U B)/(P(A)*P(B))
			
		+ 利用**卡方检验**进行相关分析

	+ **模式评估度量比较**
		+ 全置信度：min{P(A|B), P(B|A)}
		+ 最大置信度：max{P(A|B), P(B|A)}
		+ Kulczynski：0.5*(P(A|B)+P(B|A))
		+ 余弦：sqrt(P(A|B)*P(B|A))
		+ 不平衡比IR：(|A - B|)/(A+B-(A U B))
		
## 特征处理
> 单个原始特征(或变量)通常属于以下几类之一：
>    * 连续(continuous)特征
>    * 无序类别(categorical)特征
>    * 有序类别(ordinal)特征

### 初级篇
+ **连续特征**
	+  除了归一化（去中心，方差归一），不用做太多特殊处理，可以直接把连续特征扔到模型里使用。
	
+ **无序特征**
	+ 可以使用One-hot（也叫One-of-k）的方法把每个无序特征转化为一个数值向量。比如一个无序特征color有三种取值：red，green，blue。那么可以用一个长度为3的向量来表示它，向量中的各个值分别对应于red -> [1,0,0]；green -> [0,1,0]；blue -> [0,0,1]。前面说的表达方式里有一个维度是可以省略的。既然我们知道color一定是取3个值中的一个，那么我们知道向量的前两个元素值，就能推断第3个值是多少。所以，其实用下面的方式就可以表达到底是哪种颜色：red -> [1,0]；green -> [0,1]；blue -> [0,0]。这样表达的好处是少用了一个维度，降低了转化后特征之间的相关性。但在实际问题中特征基本都或多或少会有些缺失。使用第一种表达方式就可以用全0的向量来表示值缺失，而第二种表达方式是没法表达缺失的。

+ **有序特征**
> 有些特征虽然也像无序特征那样只取限定的几个值，但是这些值之间有顺序的含义。例如一个人的状态status有三种取值：bad, normal, good，显然bad < normal < good。
> 当然，对有序特征最简单的处理方式是忽略其中的顺序关系，把它看成无序的，这样我们就可以使用处理无序特征的方式来处理它。在实际问题中，这种处理方式其实用的很多。

> 当然有些问题里有序可能会很重要，这时候就不应该把其中的顺序关系丢掉。一般的表达方式如下：

| status取值 | 向量表示 |
| :------: | :------: |
| bad | (1,0,0) |
| normal | (1,1,0) |
| good | (1,1,1) |

> 上面这种表达方式很巧妙地利用递进表达了值之间的顺序关系。


### 中级篇
> 最容易让人掉以轻心的，往往就是大家觉得最简单的事。在特征处理中，最容易让刚入门同学忽略的，是对连续特征的处理方式。

> 以线性分类器Linear Regression (LinearReg)为例，它是通过特征的线性加权来预测因变量y：y = w<sub>T</sub>x。但大部分实际情况下，y与x都不会是这么简单的线性关系，甚至连单调关系都不会有。举个只有一个特征的例子，如果y与x的实际关系如下图：

![y与x的实际关系](https://i.loli.net/2019/04/25/5cc1baf538605.jpg)

> 那么直接把x扔进LinearReg模型是怎么也得不到好结果的。很多人会想着既然线性分类器搞不定，那就直接找个非线性的好了，比如高斯核的SVM。我们确实可以通过这种简单换算法的方式解决这个简单的问题。但对于很多实际问题（如广告点击率预测），往往特征非常多，这时候时间约束通常不允许我们使用很复杂的非线性分类器。这也是为什么算法发展这么多年，广告点击率预测最常用的方法还是Logistic Regression (LogisticReg)。
> 对于上面这个问题，有没有什么办法使得LinearReg也能处理得不错？**当然是有，就是对原始特征x做转化，把原来的非线性关系转化为线性关系。**

+ 方法一：离散化
	+ 最常用的转化方式是对x做离散化(discretization)，也就是把原来的值分段，转化成一个取值为0或1的向量。原始值落在某个段里，向量中此段对应的元素就为1，否则为0。**离散化的目标是y与转化后向量里的每个元素都保持比较好的线性关系**。
	+ 比如取离散点{0.5,1.5,2.5}，通过判断x属于(−∞,0.5)，[0.5,1.5)，[1.5,2.5)，[2.5,+∞)中哪段来把它离散化为4维的向量。下面是一些例子的离散结果：

| 原始值x | 离散化后的值 |
| :------: | :------: |
| 0.1 | (1,0,0,0) |
| 1.3 | (0,1,0,0) |
| 3.2 | (0,0,1,0) |
| 5.8 | (0,0,0,1) |

	+ 离散化方法的关键是怎么确定分段中的离散点。下面是常用的选取离散点的方法：
		+ **等距离离散**：顾名思义，就是离散点选取等距点。我们上面对x取离散点{0.5,1.5,2.5}就是一种等距离散。
		
		+ **等样本点离散**：选取的离散点保证落在每段里的样本点数量大致相同。
		
		+ **画图观察趋势**：以x为横坐标，y为纵坐标，画图，看曲线的趋势和拐点。通过观察下面的图我们发现可以利用3条直线（红色直线）来逐段近似原来的曲线。把离散点设为两条直线相交的各个点，我们就可以把x离散化为长度为3的向量。这种离散化为0/1向量的方法有个问题，它在离散时不会考虑到具体的x到离散边界的距离。比如等距离散中取离散点为{0.5,1.5,2.5}，那么1.499，1.501和2.49分别会离散为(0, 1, 0, 0)，(0, 0, 1, 0)和(0, 0, 1, 0)。1.499和1.501很接近，可是就因为这种强制分段的离散导致它们离散的结果差距很大。针对上面这种硬离散的一种改进就是使用**软离散**，也就是在离散时考虑到x与附近离散点的距离，离散出来的向量元素值可以是0/1之外的其他值。
		
![画图观察趋势](https://i.loli.net/2019/04/25/5cc1bdace800b.jpg)	
		
+ 方法二：函数变换
> **函数变换**直接把原来的特征通过非线性函数做变换，然后把原来的特征，以及变换后的特征一起加入模型进行训练。常用的变换函数见下表，不过其实你可以尝试任何函数。 

| 常用非线性函数f(x) | x的取值范围 |
| :------: | :------: |
| x<sub>a</sub>;a∈(-∞,+∞) | (-∞,+∞) |
| log(x) | (0,+∞) |
| log(x/(1-x)) | (0,1) |

> 这个方法操作起来很简单，但记得对新加入的特征做归一化。对于我们前面的问题，只要把x2，x3也作为特征加入即可，因为实际上y就是x的一个三次多项式。

### 高级篇

+ **笛卡尔乘积**

> 我们可以使用笛卡尔乘积的方式来组合2个或更多个特征。比如有两个类别特征color和light，它们分别可以取值为red，green，blue和on, off。这两个特征各自可以离散化为3维和2维的向量。对它们做笛卡尔乘积转化，就可以组合出长度为6的特征，它们分别对应着原始值对(red, on)，(red, off)，(green, on)，(green, off)，(blue, on)，(blue, off)。
> 对于3个特征的笛卡尔乘积组合，可以表达为立方的形式。更多特征的组合依次类推。这个方法也可以直接用于连续特征与类别特征之间的组合，只要把连续特征看成是1维的类别特征就好了，这时候组合后特征对应的值就不是0/1了，而是连续特征的取值。

+ **离散化续篇**

> 在上节中我已经介绍了一些常用的离散化单个连续特征的方法，其中一个是**画图观察趋势**。画图观察趋势的好处是直观、可解释性强，坏处是很麻烦。当要离散化的特征很多时，这种方法可操作性较差。

> 机器学习中有个很好解释，速度也不错的模型——决策树模型。大白话说决策树模型就是一大堆的if else。它天生就可以对连续特征分段，所以把它用于离散化连续特征合情合理。我称这种方法为决策树离散化方法。例如Gmail在对信件做重要性排序时就使用了决策树离散化方法2。

> 决策树离散化方法通常也是每次离散化一个连续特征，做法如下：单独用此特征和目标值y训练一个决策树模型，然后把训练获得的模型内的特征分割点作为离散化的离散点。

> 这种方法当然也可以同时离散化多个连续特征，但是操作起来就更复杂了，实际用的不多。

+ **核方法**
> 核方法经常作为线性模型的一种推广出现。

## 聚类

### DBSCAN
> 一种基于高密度连通区域的基于密度的聚类

+ **簇过程**
	+ 1. 给定数据集中所有对象都标记为"unvisited"。DBSCAN随机地选择一个未访问对象p，标记p为“visited”，并检查p的e-邻域是否至少包含MinPts对象，如果不是，标记p为**噪声点**。否则为p(p称为核心对象)创建一个新的簇C，并且把p的e-邻域中的所有对象都放到候选集合N中。
	+ 2. DBSCAN迭代地把N中不属于其他簇的对象添加到C中，在此过程中，对于N中标记为“unvisited”的对象p'，DBSCAN把它标记为"visited"，并且检查它的e-邻域。如果p'的e-邻域至少有MinPts个对象，则p'的e-邻域中的对象都被添加到N中。DBSCAN继续添加对象到C，直到C不能再扩展，即直到N为空，此时，簇C完全生成，被输出。
	+ 3. DBSCAN从剩下的对象中随机选择一个未访问的对象，聚类过程继续，直到所有对象都被访问。

### 谱聚类
+ **Ng**-**Jordan**-**Weiss算法**
	+ 1. 使用距离度量计算**相似矩阵W**；
	+ 2. 定义一个对角矩阵D，其中D<sub>ii</sub>是W第i行之和，使用相似矩阵W，导出矩阵A=D<sub>-1/2</sub>WD<sub>-1/2</sub>
	+ 3. 找出A的前k个特征向量，k应该比原数据空间的维度小得多；
	+ 4. 使用前k个最大特征值的特征向量，把原数据投影到由前k个特征向量定义的新空间，并运行聚类算法找出k个簇。
	+ 5. 根据变换后的点被分配到第4步得到的簇，把原数据点分配到这些簇。

## 离群点检测
> 又称异常检测，找出其行为很不同于预期对象的过程，这种对象称为离群点或异常。
> 离群点不同于噪声数据，噪声是被观测变量的随机误差或方差。

+ **离群点的类型**
	+ 全局离群点
	+ 情境离群点：关于对象的特定情境，它显著地偏离其他对象。
	+ 集体离群点：一些对象显著地偏离整个数据对象。
	
+ **离群点检测方法**
	+ 统计学方法
		+ 参数方法：假定正常数据对象服从某种参数分布。
			+ 基于正态分布的一元离群点检测：仅涉及一个属性或变量的数据为一元数据。使用最大似然检测一元数据，并把低概率的点识别为离群点。
			+ 多元离群点检测：把多元离群点检测任务转换成一元离散点检测问题。使用马哈拉诺比斯距离检测多元离群点。或使用卡方统计量的多元离群点检测。
			+ 使用混合参数分布检测多元离群点：使用EM算法学习参数
			+ 使用多个簇检测多元离群点
			
		+ 非参数方法：没有假定先验
			+ 使用直方图检测离群点
			+ 使用核密度估计来估计数据的概率密度分布
			
	+ 基于邻近性的方法
		+ 基于距离的离群点检测和嵌套循环方法
		+ 基于距离的离群点检测的基于网格的方法
		+ 基于密度的离群点检测
	
	+ 基于聚类的方法
		+ 把离群点检测为不属于任何簇的对象
		+ 使用最近簇的距离的基于聚类的离群点检测
		+ 通过基于聚类的离群点检测进行入侵检测
		+ 检测小簇中的离群点
		
	+ 基于分类的方法
		+ 使用一类模型检测为离群点：构建一个仅描述正常类的分类器
		+ 通过半监督学习检测离群点：使用基于聚类的方法进行聚类，然后利用已知类别分析离群点。
		
	+ 挖掘情境离群点和集体离群点
		+ 把情境离群点检测转换为传统的离群点检测
		
	+ 高维数据中的离群点检测
		+ 扩充的离群点检测：使用传统的检测方法，修正其它度量方式。
		+ 发现子空间中的离群点
		+ 高维离群点建模
			+ 基于角的离群点
			
			
			
			
			
			
			
			
			
			
			
			
			
			
		



	