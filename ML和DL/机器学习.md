
## 入门机器学习后的“随笔”

- [目录](#)
	- [偏差和方差](#偏差和方差)
	- [欠拟合underfitting](#欠拟合underfitting)
	- [过拟合overfitting](#过拟合overfitting)	
	- [梯度消失和梯度爆炸](#梯度消失和梯度爆炸)	
	- [归一化](#归一化)	
	- [贝叶斯分类方法](#贝叶斯分类方法)
	- [海量离散特征用简单模型,少量连续特征用复杂模型](#海量离散特征用简单模型少量连续特征用复杂模型)
	- [Logistical Regression](#LogisticalRegression)	
	- [SVM](#SVM)	
	- [LR和SVM的异同](#LR和SVM的异同)
	- [EM算法](#EM算法)
	- [隐马尔科夫模型](#隐马尔科夫模型)
	- [决策树](#决策树)
	- [集成方法](#集成方法)	
	- [Random Forest](#RandomForest)	
	- [GBDT](#GBDT)		
	- [XGBoost](#XGBoost)	
	- [Gradient Descent](#GradientDescent)
	- [贝叶斯信念网络](#贝叶斯信念网络)
	- [使用频繁模式分类](#使用频繁模式分类)
	- [L1、L2正则](#L1L2正则)	
	- [Metrics](#Metrics)	
	- [选择模型](#选择模型)	
	- [机器学习中的"稳定性"](#机器学习中的稳定性)	

	
	
----
	
### 偏差和方差
+ 偏差：算法期望预测和真实预测之间的偏差程度。反应的是模型本身的拟合能力。
+ 方差：度量了同等大小的训练集的变动导致学习性能的变化，刻画了数据扰动导致的影响。
-----

### 欠拟合underfitting
+ **解决措施**
	+ 添加其它特征项（增大数据量）
	+ 添加多项式特征（增加网络层数）
	+ 减少正则化参数
	
### 过拟合overfitting
+ **解决措施**
	+ 重新清洗数据
	+ 增大数据的训练量
	+ 采用正则化方法
	+ 采用dropout法
	+ 提前终止训练
	+ 减少网络层数

### 梯度消失和梯度爆炸
+ **原理**
	+ 梯度消失：当网络层数很深时，如果梯度小于1，则会使得经过多层后向反馈后的梯度累乘远远小于1；
	+ 梯度爆炸：与梯度消失相反，当网络层数很深时，如果梯度大于1，则会使得经过多层后向反馈后的梯度累乘远远大于1；

+ **解决措施**
	+ 1. 激活函数的选择（使用ReLU代替sigmoid、tanh等）
	+ 2. 预训练 + 微调
	+ 3. 使用Batch Normalization
	+ 4. 使用残差网络结构
	+ 5. 使用LSTM网络
		+ 为什么LSTM比RNN更能解决梯度消失的问题？
			+ 因为在RNN中，BPTT的梯度是累乘形式，而RNN的输出中采用了tanh激活函数，所以会出现梯度消失问题；而LSTM的梯度除了累乘形式，还有累加形式，所以不容易出现梯度消失。
		+ LSTM中用sigmoid激活函数，而不用ReLU激活函数的原因？
			+ 因为在LSTM中，忘记门和更新门是起筛选作用，所以需要0~1之间的值作为概率来进行筛选。
	+ 6. 梯度剪切、权重正则

----
	
### 归一化
+ **机器学习为什么对数据进行归一化？**
	+ 归一化的目的：
		+ 处理不同规模和量纲的数据，使其缩放到相同的数据区间和范围，以减少规模、特征、分布差异对模型的影响。
		+ 归一化加速GD求解最优解的速度。比如收敛路径呈Z字型，导致收敛太慢；
		+ 归一化可能提高精度。
	
+ **机器学习什么情况下对数据进行归一化？**
	+ 使用了梯度下降算法，如LR、SVM等；
	+ 计算样本点距离时，如KNN、K-Means等。
	+ ......
+ **机器学习什么情况下不需要归一化？**
	+ 概率模型（决策树）不需要归一化。

+ **常用的归一化方法**
	+ max-min法：容易受极端值的影响，一定程度上会破坏原有的数据结构；
	+ z-score法：会改变原有数据的分布，不适合对稀疏数据做处理，不适合根据变量差异程度的聚类分析；
	+ RobustScaler：适用于存在离群点的数据。
	+ 上述方法分析：在分类中，聚类算法，数据符合正态分布中，需要使用距离来度量相似性或者使用PCA降维时，z-score表现得较好。在不涉及距离测量，协方差计算，数据不太符合正态分布时，可以使用第一种方法或其他方法。

+ **LR归一化问题，什么情况下可以不归一化，什么情况下必须归一化，为什么？**
	+ ......
-----

### 贝叶斯分类方法
+ **贝叶斯定理**
	+ 后验概率：P(H/X)，在条件X下，H的后验概率；P(X/H)，在条件H下，X的后验概率；
	+ 先验概率：P(H)，H的先验概率；P(X)，X的先验概率。
	+ 定理：P(H/X) = (P(H)*P(X/H)) / P(X)

+ **朴素贝叶斯分类**
	+ 前提条件：
		+ 基于贝叶斯定理；
		+ 特征条件独立假设：用于分类的特征在类确定的条件下都是条件独立的。**朴素的原因**
	+ 后验概率最大化准则：max P(H/X)，相当于期望风险最小化。
	+ 遇零概率值的处理方法：拉普拉斯平滑，即在极大似然估计计算中，分子加1，分母加K，K为类别数。
	+ 对于连续值属性
		+ 假定连续值属性服从均值为u，标准差为a的高斯分布。
	

----

### LogisticalRegression
+ **LR作为线性模型，如何拟合非线性情况？**
	+ **特征侧**：离散化、交叉组合
	+ **模型**：引入kernel，或推广到FM等model。

+ **提到LR损失函数要能知道交叉熵，为什么使用交叉熵？使用交叉熵为损失函数的优化问题是在优化什么量？交叉熵和KL散度、相对熵的关系？**
	+ ......
	+ ......

+ **LR的分布式实现逻辑是怎样的？数据并行和模型并行的区别？P-S架构是什么东西？**
	+ ......
	+ ......

-----
### 海量离散特征用简单模型少量连续特征用复杂模型。
-----

### SVM
+ **原理**
	+ 最大间隔分离超平面；
	+ 最小化合页损失函数Hinge loss；

+ **优缺点**
	+ 解决小样本情况下的机器学习问题；
	+ 提高泛化能力；
	+ 处理高维空间数据；
	+ 解决非线性问题；
	+ 对于线性问题没有通用的解决方案，谨慎选择kernel函数；
	+ 处理分类问题时，要求解函数的二次规划问题，需要大量的存储空间。

+ **SVM中假设分离超平面系数非常大怎么办？**
	+ ......

+ **针对数据和特征的关系，如何选择SVM核函数？**
	+ 1. 特征数量和样本数量差不多时，选用LR或线性核的SVM；
	+ 2. 特征数量少、样本数量正常时，选用SVM+高斯核函数；
	+ 3. 特征数量少、样本数量很大时，需要手工添加一些特征变成第一种情况；
	+ 4. 特征数量少、样本数量少时，选用SVM + 非线性核。

+ **SVM常用核函数**
	+ linear核：主要用于线性可分情况，参数少，速度快；
	+ RBF核：主要用于线性不可分情况；（**注意：linear核是RBF核的一种特例**）
	+ sigmoid核

+ **常用核函数选择**
	+ 可以先用linear核函数看看效果，不好再用RBF核函数；
	+ 一般RBF核函数的性能位于linear核和sigmoid核之间，这也是为什么一般选用RBF核的原因。
	+ 如果对样本没有先验信息，则可以利用cross validation方法来评估不同核函数的性能，再选出性能最好的核函数。

+ **Pegasos算法**
> 《机器学习实战》——第15章
> SMO算法一次优化两个支持向量，并在整个数据集上迭代，在需要注意的值上停止，不容易并行化。
> Pegasos(Primal Estimated sub-Gradient Solver)指原始估计梯度求解器，该算法使用某种形式的随机梯度下降方法来解决SVM所定义的优化问题，研究表明该算法所需的迭代次数取决于用户所期望的精确度而不是数据集的大小。
> Pegasos算法可以看成是SMO算法的替代，容易写成MapReduce的形式。
	
+ 工作流程：从训练集中随机挑选一些样本点添加到待处理列表中，之后按序判断每个样本点是否被正确分类；如果是，则忽略；如果不是，则将其加入到待更新集合。批处理完毕后，权重向量按照这些错分的样本进行更新。
	+ 伪代码：
		```python
		将w初始化为0
		对每次批处理
			随机选择k个样本点（向量）
			对每个向量
				如果该向量被错分：
					则更新权重向量w
			累加对w的更新
			
		# SVM的Pegasos算法
		def predict(w, x):
			return w*x.T
			
		def batchPegasos(dataSet, labels, lam, T, k):
		'''
			Args：
				dataSet：特征集合
				labels：分类结果集合
				lam：固定值
				T：迭代次数
				k：待处理列表大小
				
			Returns：
				w：回归系数
			
		'''
			m, n = shape(dataSet)
			w = zeros(n)
			dataIndex = range(m)
			for t in range(1, T+1):     # T次迭代中，每次需要重新计算eta
				wDelta = mat(zeros(n))  # 重置wDelta
				eta = 1.0/(lam*t)     # 类似学习率，权重调整幅度的大小
				random.shuffle(dataIndex)
				for j in range(k):      # 全部的训练集  内循环中执行批处理，将分类错误的值全部做累加后更新权重向量
					i = dataIndex[j]
					p = predict(w, dataSet[i, :])  # mapper代码(分布式中)
					if labels[i]*p < 1:      # 如果预测正确，并且预测结果的绝对值大于等于1，因为最大间隔为1，则预测正确；否则预测错误，通过预测错误的结果来累计更新w
						wDelta += labels[i]*dataSet[i, :].A    # 累计变化
				w = (1.0 - 1/t)*w + (eta/k)*wDelta             # 在每个T上应用更改
			return w
					
		
		```
-----

### LR和SVM的异同
+ **同：**
	+ 都是监督学习算法，都是判别模型；
	+ 都可以处理分类问题，一般都处理线性分类问题（**注意：LR也可以使用核函数**）；
	+ 两者都可以使用不同的正则化项，并在很多实验中，两者性能相当。
	
+ **异：**
	+ 损失函数不同，一个是Logistical loss，一个是Hinge loss；
	+ LR是参数模型，对异常值敏感；SVM是非参数模型；
	+ LR训练时考虑所有数据，容易受数据不平衡的影响；而SVM只依赖于支持向量，但是由于SVM基于间隔分类，一般要对数据做归一化处理。
	+ 对于非线性问题，SVM通常采用核函数解决，而LR很少用。这是因为使用核函数，SVM只要支持向量参与计算，而LR则是所有数据都参与计算，复杂度高。

----

### EM算法
> EM算法属于无监督学习算法，就是含有隐变量的概率模型参数的极大似然估计，或极大后验概率。
> 像贝叶斯估计法，其概率模型的变量(X -> Y)都是观测变量，给定观测数据，可以直接由极大似然估计法或贝叶斯估计法估计模型参数；而当概率模型的变量含有隐变量Z时(X -> Z -> Y)，则不能直接求解极大似然估计法。

+ **目标函数**
	+ 极大化对数似然函数：max logP(Y|\theta)=logsum(Z){P(Y,Z|\theta)}
	+ 由于目标函数没有解析解，所以需要进行转化，使用迭代的方法进行求解。

+ **求解方法**
	+ 见《统计学习方法—李航著》

----

### 隐马尔科夫模型
> 隐马尔科夫模型是关于时序的概率模型，描述由一个隐藏的马尔科夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。由以下组成：
> 1. 初始概率分布
> 2. 状态转移概率分布
> 3. 观测概率分布

+ **两个基本假设**
	+ **齐次马尔科夫性假设**：假设隐藏的马尔科夫链在任意时刻t的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关，也与时刻t无关。
	+ **观测独立性假设**：假设任意时刻的观测只依赖于该时刻的马尔科夫链的状态，与其他观测及状态无关。
	
+ **三个基本问题**
	+ **概率计算问题**：给定模型参数A和观测序列O，计算在模型参数A下观测序列O出现的概率P(O|A)；
		+ 使用**前向与后向算法**解决。
	
	+ **学习问题**：已知观测序列O，估计模型参数A，使得在该模型下观测序列概率P(O|A)最大。用极大似然估计的方法估计参数A。
		+ 使用**监督学习算法**解决：已知观测序列和对应的状态序列。
		+ 使用**非监督学习算法**解决：Baum-Welch算法（即EM算法）
		
	+ **预测问题**：已知模型参数A和观测序列O，求给定观测序列条件概率P(I|O)最大的状态序列I，即给定观测序列，求最可能的对应的状态序列。
		+ 使用**近似算法和维特比算法**解决。
	
-----

### 决策树
+ **ID3算法**：在决策树各个结点上应用**信息增益准则**选择特征，递归地构建决策树。该决策树是多分支分类。

	+ **信息增益**
		+ 意义：给定特征X的条件下，使得类别Y的信息的不确定性减少的程度。**取值越大越好**。
		+ 定义：集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D/A)之差。
	+ **缺点**
		+ 1. 分支过程中偏向取值较多的属性；
		+ 2. 只能处理离散值；
		+ 3. 对缺失值敏感。

+ **C4.5算法**：C4.5算法与ID3算法类似，C4.5算法使用**信息增益比**来选择特征。C4.5算法先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择信息增益比最大的属性。该决策树是多分支分类。
	+ **信息增益比**
		+ 定义：在信息增益的基础上，再除以H(D)；**取值越大越好**。
		
	+ **连续属性的划分**：采用“二分”法对连续属性进行离散化，划分点的选取可选使信息增益最大化的划分点。例：16个连续属性值选15个划分点。
		
	+ **缺点**
		+ 1. 分支过程中偏向取值较少的属性；
		+ 2. 适合小样本；
		+ 3. 要进行剪枝操作；要对属性进行排序。
			
+ **CART树**：CART树既可用于分类，也可用于回归。CART树属于二叉树。
	+ **回归树**：使用平方误差来构建决策树，使用：min(J){min(c1)sum(y-c1)^2+min(c2)sum(y-c2)^2}来选择**最优划分变量和最优划分点**。
		
	+ **分类树**：使用**基尼系数**选择最优特征。
		+ **基尼系数**
			+ 定义：从数据集中随机抽取两个样本，其类别标记不一致的概率。基尼系数越小，则样本集合的不确定性越小。
			+ 公式：1-sum(K){P(k)*P(k)}，P(k)属于第k个类别的概率，共有K个类别。
				
		+ **缺点**
			+ 1. 适合大样本；
				
+ **预剪枝**
	+ 过程：进行分支前，计算验证集准确率；分支后，计算验证集准确率，若变大，则进行分支，反之。
	+ 缺点：欠拟合风险较高。
	
+ **后剪枝**
	+ 过程：当前决策树计算非叶子结点在验证集上的准确率，将该非叶子结点替换为叶子结点后，计算验证集的准确率，若变大，则进行剪枝，反之。
	
+ **决策树对缺失值的处理**
	+ 1. 删除缺失数据；
	+ 2. 用其它值猜测缺失项的可能值，如中位数、众数等，或者用已有数据构建模型，然后对缺失值进行预测；
	+ 3. 概率化：C4.5算法中，按比例对所有样本分配权重；
	+ 4. XGBoost中，将缺失值分别导流到各个分支中，然后计算每个分支对损失函数的影响，将该缺失值分配到使得损失函数最小的分支。
	
+ **树模型的优缺点**
	+ **优点**
		+ 1. 可解释性强；
		+ 2. 可处理混合类型特征；
		+ 3. 不需归一化；
		+ 4. 有特征组合、特征选择的作用；
		+ 5. 能够处理缺失值；
		+ 6. 对异常点鲁棒；
		+ 7. 可扩展性强，容易并行。
	
	+ **缺点**
		+ 1. 缺乏平滑处理（回归预测的输出值只能输出若干种值）；
		+ 2. 不适合处理高维稀疏数据。
		
	
----

### 集成方法
+ 关注降低variance，选择bias较小的基学习器
	+ **Bagging**
		+ 给定m个样本的数据集，利用有放回的随机采样法，得到T个含有m个样本的训练集，然后训练基学习器得到T个基学习器，对分类任务采用**投票法**，对回归任务采用**平均法**。
		+ **每个基学习器只使用了m个样本中约63.2%的样本，剩下36.8%的样本可用作验证集。**
		+ **样本扰动**

	+ **Stacking**
		+ 从初始数据集中训练出T个初级学习器，然后将T个初级学习器的输出当做次级学习器的样例输入，而初始样本的标签仍作为样例标记，用新得到的数据集训练次级学习器。
	
	+ **Random Forest**
		+ **样本扰动+属性扰动**
	
+ 关注降低bias，选择variance较小的基学习器
	+ **AdaBoost**
		+ 从初始数据集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器出错的训练样本在后面训练过程中得到更多的关注，然后利用调整后的样本分布来训练下一个基学习器，如此重复，最后对每轮训练得到的基学习器进行加权后相加。**（基学习器常采用回归树和逻辑回归）**

----

### RandomForest
> 在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树节点分裂时引入**随机属性**扰动。

> **随机性**体现（与传统决策树的差异）：对基决策树的每个节点，先从该节点的属性集合中随机选择包含K(log2(d))个属性的子集，然后再从这个子集中选择一个最优的属性用于划分。

+ **为什么随机选取数据集？**
	+ 如果不随机的话，训练出来的多棵树的分类结果是一样的，违背了bagging思想;

+ **为什么有放回抽样？**
	+ RF在分类时是求同，有放回的抽样会产生相同的训练样本；如果不是有放回抽样，训练出来的每棵树的结果存在很大偏差，这样对分类结果没有任何帮助。所以有放回抽样能够减小bias，RF的目的是减小variance。

+ **影响RF分类结果的因素：**
	+ 任意两棵树的相关性：相关性越大，错误率越高；
	+ 每棵树的分类能力：分类器能力越强，错误率越低；
	+ 唯一可调参数：RF中特征子集的数量，数量越大，性能越好。

+ **RF的优缺点**
	+ RF引入两个随机性，抗噪能力增强；
	+ RF在分类中对各个变量的重要性进行估计，对泛化误差进行无偏估计；
	+ 可处理高维数据，对数据集的适应能力强；
	+ 性能由于单预测器，分类精度与boosting算法差不多，运行速度更快；
	+ 训练数据较少或噪声数据较大时，会发生overfitting。
-----

### GBDT
+ **GBDT优缺点：**
	+ 灵活处理各种离散值和缺失值；
	+ 对异常值具有鲁棒性；
	+ 相对SVM，预测的准确率较高；
	+ 由于弱学习器之间存在依赖关系，难以并行训练；
	+ 不适合处理高维稀疏数据，缺乏平滑处理。

+ **算法流程：**
	+ 多轮迭代，每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的**残差**基础上进行训练。对弱分类器的要求一般是低variance和高bias的简单分类器，如CART树。最终分类器是将每轮训练得到的弱分类器加权求和得到的。**（采用不放回采样）**

+ **损失函数：**
	+ 一般损失函数的负梯度(一阶导)作为残差；（因为一般损失函数存在优化困难问题，所以取一阶导）

+ **选择特征：**
	+ 以CART树为例，原始GBDT遍历每个特征，然后对每个特征遍历它所有可能的切分点，找到最优特征J和最优切分点，即 min(J){min(c1)sum(y-c1)^2+min(c2)sum(y-c2)^2}。

+ **分裂节点的评价指标：**
	+ 分裂时选择使得误差下降最多的分裂。

+ **构建特征：**
	+ 利用GBDT产生特征的组合(CTR预估中，工业届一般用LR处理非线性数据，其中就可以用这种特征组合的方式增强LR对非线性分布的拟合能力)。比如，利用GBDT生成两棵树，根据叶子结点的个数生成一个向量，然后将样本分别输入进去，样本分到叶子结点处该向量位置值为1，其余为0，根据得到的这个向量作为该样本的组合特征。

+ **用于分类：**
	+ GBDT无论用于分类还是回归都是用CART树。对于K分类问题，每次迭代构建K棵树，然后利用softmax求出属于每棵树的概率，残差是真实label与第k棵树的概率的差值。

+ **GBDT减少误差的方式：**
	+ 每棵树拟合当前模型预测值和真实值之间的误差。

+ **正则化：**
	+ **学习率**：越小越易overfitting；
	+ **子采样比**：越小越易underfitting
	+ **剪枝操作**

+ **参数**
	+ **步长、学习率**：减小步长，提高泛化能力；学习率过大容易overfitting；
	+ **树最大深度**：太大overfitting
	+ **内部节点再划分所需最小样本数和叶子节点最少样本数**：内部节点再划分所需最小样本数越大越防overfitting
	+ **子采样比**：太小容易underfitting

+ **相比于SVM、LR，GBDT的优势：**
	+ 基于树模型，继承了树模型的优点
		+ 对异常点鲁棒；
		+ 不相关特征干扰性低；
		+ 能处理缺失值；
		+ 受噪声干扰小。

+ **高维稀疏特征的时候，LR的效果为什么会比GBDT好？**
	+ 因为LR等线性模型的正则项是对权重进行惩罚(**L1正则**：使得某些权重值为0，减少稀疏特征，降低运算量)，而树模型的惩罚项通常为叶子结点数量和深度。因此带正则化的线性模型不容易overfitting。

+ **为什么GBDT、XGBoost每棵树的深度很浅，而Decision Tree、RF每棵树的深度很深？**
	+ GBDT关注降低bias，每棵树浅带来的variance低；
	+ RF关注降低variance，每棵树深带来的bias低。

+ **GBDT和Adaboost的区别**
	+ **异：**
		+ GBDT是利用残差来训练每棵decision tree；Adaboost通过误差来调整每个样本的权重，从而训练每棵decision tree。
		+ 损失函数不同：GBDT使用一般损失函数；Adaboost使用指数损失函数。

	+ **同：**
		+ 都可以用来分类，相对于经典判别分类方法，效果好；
		+ 由多棵树组成。
		
### XGBoost
+ **为什么XGBoost要以二阶导数值作为权重进行分位？**
	+ 从目标函数可以看出，二阶导数值对 loss 有加权作用。
	
+ **XGBoost如何处理稀疏值和缺失值？**

	+ 当特征出现稀疏值时，不应该转换为numpy等形式。
		+ 把数据转换成libSVM数据格式：<br>Label index1:value1 index2:value2 ...</br>其中，index是以1开始的整数，可以不连续；value就是特征变量的取值，label就是对应的标签. 
		+ 加载numpy的数组到DMatrix对象中
	+ 当特征出现缺失值时，XGBoost可学习出默认的节点分裂方向。将缺失值向左、右节点各分一次，计算最大的Gain，向Gain最大的方向分。
	
+ **XGBoost的其它特性**
	+ 行抽样、列抽样（借鉴Random Foreast）
	+ Shrinkage，即学习率，减小学习率，迭代次数增加，有正则化作用。（减小学习率，相当于每个叶节点分数变小）
	+ 支持自定义损失函数（需二阶可导）。
----
	
+ **GBDT和XGBoost的区别**
	+ XGBoost支持线性分类器，传统GBDT以CART作为基分类器(**GBDT支持LR**)；XGBoost支持线性分类器，相当于带L1和L2正则化的LR或线性回归。
	+ GBDT对损失函数求一阶导，而XGBoost对损失函数进行二阶泰勒展开；XGBoost支持自定义代价函数，代价函数需一阶和二阶可导；
	+ XGBoost的代价函数里自带正则项，用于控制模型的复杂度；
	+ XGBoost支持列抽样，借鉴了RF的做法，能降低overfitting，减少计算；(**类似sklearn里gbm的max_feature参数**)
	+ XGBoost工具支持并行，并行是在特征上做的，GBDT每次节点分裂时，需要对所有特征计算并排序，而XGBoost预先对数据排序并保存为block结构（预排序算法，计算特征增益），迭代过程中重复使用这个结构，大大减少计算量。在节点分裂时，特征增益的计算可以开多线程实现。在内存有限或分布式的情况下，数据无法一次性载入时，XGBoost用可并行的近似直方图算法能高效生成候选的分割点。
-----

### GradientDescent
+ **BGD、SGD、MBGD的区别？**
	+ BGD：批量梯度下降法，每次迭代需要所有的样本，该方法可以得到全局最优解，但是会影响速度。
	+ SGD：随机梯度下降法，每次迭代使用一个样本，迭代次数多。该方法带来的噪声较多，非全局最优解，使得每次迭代并不是朝整体最优化方向，可能使准确率下降。但是训练速度快。
	+ MBGD：小批量梯度下降法，每次迭代需要部分样本。该方法是对上述两种方法的改进。
+ **不同的GD方法有什么区别和联系？**
	+ ...

+ **二阶优化算法有什么？**
	+ ...

+ **对比off-line和on-line learning的区别？**
	+ ...
----

### 贝叶斯信念网络
> 又称信念网络、贝叶斯网络、概率网络

+ **原理**：贝叶斯信念网络说明联合条件概率分布。由两个成分定义——**有向无环图**和**条件概率表**的集合。有向无环图的每个节点代表一个随机变量，变量可能对应于给定数据中的实际属性，每条弧表示一个概率依赖，如果一条弧由节点Y到Z，则Y是Z的双亲或直接前驱，而Z是Y的后代。给定其双亲，每个变量条件独立于图中它的非后代。对于每个变量，给出条件概率表。

+ **训练贝叶斯信念网络**
	+ 网络拓扑可由专家构造或由数据导出。网络变量可以是可观测的，或隐藏在训练数据中，隐藏数据的情况称缺失值或不完全数据。
	+ 如果网络拓扑已知且变量是可观测的，则训练网络是直接的，该过程由计算条件概率表的表目组成，与朴素贝叶斯分类涉及的概率计算类似。
	+ 当网络拓扑给定，而某些变量是隐藏时，可以选择不同方法来训练信念网络。这里介绍有希望的梯度下降法。权重初始化为随机概率值。梯度下降策略采用贪心爬山法，在每次迭代后，这些权重都会被修改，并最终收敛到一个局部最优解。
		+ 优化问题：最大化联合概率问题
		+ 计算梯度：先对联合概率求自然对数，然后对权重求偏导
		+ 更新权重
		+ 重新规格化权重：由于权重值是概率值，并且所有权重值的和为1，所以取值范围为0.0和1.0之间.
    ```        
    建立过程
    1、计算每个属性值的概率分布和条件概率分布；
    2、根据第一步的两种概率分布计算任意两个节点的互信息作为两个节点连接边的权值；
    3、计算最大权生成树
        a、初始状态：n个节点，0条边
        b、插入最大权重的边
        c、找到下一个最大的边，并且加入到网络中，要求没有环生成，否则，查找第二大的边；
        d、重复c过程直到插入了n-1条边，网络建立完成;
    4、选择任意节点作为根，从根到叶子标识边的方向；
    5、可以保证这颗树的近似联合概率和贝叶斯网络的联合概率的相对熵最小。
    ```

----

### 使用频繁模式分类
+ **关联分类**
> 关联规则分类包括以下步骤：
>	1. 挖掘数据，得到频繁项集，即找出数据中经常出现的属性-值对；
>	2. 分析频繁项集，产生每个类的关联规则，它们满足置信度和支持度标准；
>	3. 组织规则，形成基于规则的分类器。
> 关联分类方法的主要不同在于挖掘频繁项集所用的方法、如何将被分析的规则导出并用于分类。

	+ **基于分类的关联CBA**：实验表明，在大量数据集上比C4.5更准确。
	+ **基于多关联规则的分类CMAR**
	+ **基于预测关联规则的分类CPAR**

+ **基于有区别力的频繁模式分类**

----

### L1L2正则
+ **关于L1和L2正则，其几何解释和概率解释**
	+ 从Bayes角度来看，L1、L2正则相当于对模型参数引入先验分布：
		+ L1正则：模型参数服从拉普拉斯分布，对参数加入分布约束，大部分取值为0。
			+ 特征选择：稀疏性（权值稀疏）
			+ 鲁棒性：忽略异常点
		+ L2正则：模型参数服从高斯分布，对参数加了分布约束，大部分取值很小。
			+ 解决过拟合
			+ 易优化和计算（权值平滑）
			+ 稳定性好
			+ 对异常点敏感：误差取平方后放大。
	+ 稳定性比较解释
		+ L1存在**ill condition（病态）问题**：输入发生微小变化导致输出发生很大改变。
	+ 稀疏解和平滑解解释
		+ 从目标函数看，等高线与约束条件首次相交的地方为最优解。
			+ L1产生稀疏性![L1产生稀疏性](https://i.loli.net/2019/04/12/5cb05448ba10e.jpg)
			+ L2产生平滑解![L2产生平滑解](https://i.loli.net/2019/04/12/5cb05495ae346.jpg)
			
		+ 从优化角度看：
			+ ![L1和L2的更新量](https://i.loli.net/2019/04/12/5cb0554600a16.jpg)
			+ L1的权值更新量：w<sub>(i+1)</sub> = w<sub>i</sub> - \eta
			+ L2的权值更新量：w<sub>(i+1)</sub> = w<sub>i</sub> - \eta * w<sub>i</sub>
			
	

----

### Metrics

| 预测<br><br>实际| 1  | 0 |    |
| :--------: | :--------: | :--------: |:--------:|
| 1  |   TP   |   FN  |   TP+FN  |
| 0  |   FP   |   TN  |   FP+TN  |
|    | TP+FP  | FN+TN | TP+FN+FP+TN |

+ **准确率**：（TP+TN）/（TP+TN+FP+FN）
+ **错误率**：（FP+FN）/（TP+TN+FP+FN）
+ **召回率(Recall)、查全率**：实际的正样本中，分类成正样本的比例，（TP）/（TP+FN）
+ **精确率(Precision)、查准率**：分类成正样本中，实际的正样本的比例，（TP）/（TP+FP）
+ **精确率和召回率的趋势呈现逆关系**

+ **真正例率（TPR）**：（TP）/（TP+FN），相当于Recall
+ **假正例率（FAR、FPR）**：（FP）/（FP+TN），也称误报率
+ **拒真率（FRR）**：实际的正样本中，分类成负样本的比例，（FN）/（TP+FN），相当于1-Recall
+ **F1分数**：精确率和召回率的调和指标，认为两个指标同等重要；
	+ 2×（precision×recall）/（precision+recall）
+ **F\_β分数**：（1+β×β）×（precision×recall）/（（β×β×precision）+recall）
+ **F2分数**：β等于2，召回率的重要程度是精确率的2倍
+ **G分数**：精确率和召回率的几何平均，sqrt(precision×recall)
----

+ **ROC曲线**
> 不平衡数据集中最常用的指标之一
> 观察模型正确识别正样本的比例与模型错误地把负样本识别为正样本的比例之间的权衡。

![ROC曲线](https://s2.ax1x.com/2019/04/06/AWLVLF.jpg "ROC曲线")

+ **几个结论：**
	+ 阈值变大，样本分类成负样本的可能性增大（FP ↓，FN ↑）；阈值变小，样本分类成正样本的可能性增大（TP、FP ↑）；
	+ 好的分类模型应该尽可能位于坐标轴的左上方；随机猜测模型应该靠近于（0,0）和（1,1）这条对角线上；
	+ AUC面积值位于[0,1]，值越大说明模型分类越好；

+ **数据集平衡问题**
	+ 对于**平衡数据集**，采用**准确率**评估最有效；
	+ 对于**不平衡数据集**，采用**精确率**、**召回率**、**F分数**最有效。
	+ 针对每个元组可能具有多个标签的问题，使用准确率度量是不合适的。应该使用**每个类分布概率**来度量。
	
	+ **提高类不平衡数据的分类准确率**
		+ **权重**：对输出结果乘上一个相应的权重。
		+ **过抽样**：对类别少的样本重复采样，使得训练集具有相同的正样本数和负样本数。
		+ **欠抽样**：从类别多的样本中进行采样，使得两种类别的样本数量一致。
		+ **阈值移动**：对输出值设定一个阈值，使得类别少的样本更容易分类。
		+ **组合技术**：通过学习和组合一系列基分类器提高总体准确率。
		+ **相对来说**，**阈值移动和组合技术要优于过抽样和欠抽样**。
		
+ **针对不平衡数据，应该如何选择阈值？**
	+ 相等错误率（ERR），FAR随阈值的增大而减少，FRR随阈值的增大而增大。当FAR=FRR时，此时阈值最合适。

----

### 选择模型
> 假设已经由数据产生了两个分类模型M1和M2，已经进行10折交叉验证，得到了每个模型的平均错误率，如何确定哪个模型最好？
> 直接选择最低错误率的模型是不行的，尽管由M1和M2得到的平均错误率看上去可能不同，但是差别可能不是统计显著的，需要使用统计显著性检验。为此，希望得到平均错误率的**置信界**：对于未来样本的95%，观测到的均值将不会偏离正、负两个标准差；或者一个模型优于另一个模型，误差幅度为±4%。

#### 使用统计显著性检验
+ **t-检验**：计算K个样本具有K-1自由度的t-统计量
	+ 计算方式：(mean(err(M1)) - mean(err(M2)))/sqrt(var(M1-M2)/K)，其中mean(err(Mi))是模型Mi的平均错误率，var(M1-M2) = (1/k)*sum((err(M1)<sub>k</sub> - err(M2)<sub>k</sub> - (mean(err(M1)) - mean(err(M2))))<sup>2</sup>)
		
#### 模型稳定度
+ **群体稳定性指标PSI**：Population Stability Index，衡量测试样本及模型开发样本评分的分布差异。场景：信用评分、信贷等。其实PSI表示的就是按分数分档后，针对不同样本，或者不同时间的样本，population分布是否有变化，就是看各个分数区间内人数占总人数的占比是否有显著变化。

	+ **公式**：PSI = sum((实际占比-预期占比)×ln(实际占比/预期占比))
		+ 举例：比如训练一个logistic回归模型，预测时候会有个概率输出p。测试集上的输出设定为p1吧，将它从小到大排序后10等分，如0-0.1,0.1-0.2,......。现在用这个模型去对新的样本进行预测，预测结果叫p2，按p1的区间也划分为10等分。实际占比就是p2上在各区间的用户占比，预期占比就是p1上各区间的用户占比。意义就是如果模型跟稳定，那么p1和p2上各区间的用户应该是相近的，占比不会变动很大，也就是预测出来的概率不会差距很大。
		+ 除了按概率值大小等距十等分外，还可以对概率排序后按数量十等分，两种方法计算得到的psi可能有所区别但数值相差不大。

	+ **PSI值与模型的关系**
		+ 小于10%：无需更新模型
		+ 10%-25%：检查一下其他度量方式
		+ 大于25%：需要更新模型

	+ **变量的PSI计算**
		+ PSI：检验变量的稳定性，当一个变量的psi值大于0.0001时，变量不稳定。一个变量，将它的取值按照分位数来分组一下，每一组中测试模型的客户数占比减去训练模型中的客户数占比再乘以这两者相除的对数，就是这一组的稳定性系数psi，然后变量的psi系数就是把这个变量的所有组的psi相加总起来。
		
+ **KS曲线**
> KS曲线，KS值：学习器将正例和反例分开的能力，确定最好的“截断点”。
+ **KS曲线是把TPR和FPR都作为纵坐标，而样本数作为横坐标。**
+ **作图步骤**
	+ 1. 根据学习器的预测结果（注意，是正例的概率值，非0/1变量）对样本进行排序（从大到小）-----这就是截断点依次选取的顺序。
	+ 2. 按顺序选取截断点，并计算TPR和FPR ---也可以只选取n个截断点，分别在1/n，2/n，3/n等位置。
	+ 3. 横轴为样本的占比百分比（最大100%），纵轴分别为TPR和FPR，可以得到KS曲线。
	+ 4. TPR和FPR曲线分隔最开的位置就是最好的”截断点“，最大间隔距离就是KS值，通常>0.2即可认为模型有比较好偶的预测准确性。
+   KS值越大，表示模型能够将正、负客户区分开的程度越大。通常来讲，KS>0.2即表示模型有较好的预测准确性。	


----
		
### 机器学习中的稳定性

[链接](https://zhuanlan.zhihu.com/p/27787096)

+ **计算的稳定性**：指模型运算性能的鲁棒性。
	+ 下溢和上溢：位数超出了计算机可承载范围。
		+ 下溢：例如，x个小数相乘，则小数位可能失去精度。
		+ 上溢：例如，x个整数相乘，正数位数溢出。64位计算机的数值上限为2<sup>63</sup>-1。
		
	+ 平滑与0：拉普拉斯平滑。
	+ 算法稳定性和扰动
		+ 机器学习模型中，考虑算法对于数据扰动的鲁棒性。“模型的泛化由误差（Bias）和方差(Variance)共同决定，而高方差是不稳定性的罪魁祸首”。也就是： 输入发生微小变化，输出产生巨大变化， 则说明算法不稳定。
		+ 矩阵求逆工程不稳定，一般避开矩阵求逆。
		+ 神经网络批量学习： 错误的学习率和批量尺寸导致不稳定的学习过程。当小批量进行学习，小样本中的高方差导致学到的梯度不精确，这种情况，应该使用小学习速率。 相反，当批量尺寸选的较大，则可以使用较大的速率。
		+ 决策树：属于不稳定的模型。训练数据中微小改变可能改变决策树的结果。为了解决稳定性，出现了集成学习。**SVM模型相对稳定**。
	
+ **数据稳定性**：数据稳定性取决于其方差（Variance）。
	+ 独立同分布（IID）与泛化能力
		+ 模型能获得强泛化能力数据保证就是其训练数据是独立同步分从母体分布上采样而得。一般训练数据足够稳定,需要：1. 训练数据越多越好，降低数据中的偶然性。2. 确保训练数据和母体数据及月数据来自于一个分布。因此数据稳定性的基本前提就是： 独立同分布，且数量越多越好。 稳定数据可以保证模型的经验误差约等于泛化误差。
	+ 新常态：类别不平衡
		+ 当我们采用过采样，欠采样时，注意数据的稳定性，是否带来了过高的方差Variance。

+ **性能的稳定性**：评估机器学习模型的稳定性，和评估机器学习的表现有本质不同。不能简单通过准确率评估机器学习稳定与否。
	+ 交叉验证太慢；
	+  统计计算理论对算法进行分析：1、概率近似正确框架（PAC）；2、界限出错框架（MBF）。

		
----

