# 深度学习中的学习方法

[知乎链接](https://zhuanlan.zhihu.com/p/22252270)

- [目录](#)
	- [解决问题](#解决问题)
	- [滑动平均](#滑动平均)
	- [Momentum](#Momentum)
	- [Nesterov](#Nesterov)
	- [Adagrad](#Adagrad)
	- [RMSProp](#RMSProp)
	- [Adadelta](#Adadelta)
	- [Adam](#Adam)
	- [Nadam](#Nadam)
	
## 解决问题
> 局部极小值和鞍点会使得训练停滞，而病态曲率(Hessian矩阵)则会减慢训练速度。

![损失曲面](https://i.loli.net/2019/04/28/5cc5757fd0e28.jpg)

----
## 滑动平均
> 滑动平均，或指数加权平均：可以用来估计变量的局部均值，使得变量的更新与一段时间内的历史取值有关。

![更新公式](https://i.loli.net/2019/04/28/5cc57704b3274.jpg)

> 上式中，\beta取值一般为0.9。
----

## Momentum
> 称动量法。
+ 不仅使用当前的梯度，同时利用之前的梯度提供的信息。

![更新公式](https://i.loli.net/2019/04/28/5cc577f88ddfa.jpg)

+ **问题**
	+ 存在“冷启动”问题

----

## Nesterov
> 相比于Momentum，计算t-1时刻的梯度

![更新公式](https://i.loli.net/2019/04/28/5cc58035065e7.jpg)

----

## Adagrad
> 对学习率进行自适应约束，每个位置的更新都使用不同的学习率。

![更新公式](https://i.loli.net/2019/04/28/5cc580f75dd77.jpg)

+ 不需要手工调节学习率，但是会过早的结束训练。

----

## RMSProp
> 在Adagrad中，由于学习率分母上的变量一直在累加按元素平方的梯度，每个元素的学习率在迭代过程中一直在降低或不变。所以当学习率在迭代早期下降较快且当前解依然不理想时，**Adagrad在迭代后期较难找到一个有用的解**。
> RMSProp使用一个梯度按元素平方的指数加权平均变量v，并将其中每个元素初始化为0，每次迭代中，先计算小批量梯度g，然后对该梯度按元素平方后做滑动平均并计算v

![更新公式](https://i.loli.net/2019/04/28/5cc57a30ace92.jpg)


+ 使得学习率慢慢下降，不会出现Adagrad中出现的问题，但更容易受前一时刻的影响。


----

## Adadelta
> 为了解决Adagrad中的问题，Adadelta没有**学习率参数**

![更新公式](https://i.loli.net/2019/04/28/5cc58181c033b.jpg)

----

## Adam
> 结合动量和RMSProp的启发式算法

![更新公式](https://i.loli.net/2019/04/28/5cc57bfc8f00a.jpg)

+ 为了解决v<sub>t</sub>和s<sub>t</sub>被初始化为0在迭代初期存在“冷启动”的问题，对v<sub>t</sub>和s<sub>t</sub>进行修正
	+ v<sub>t</sub> = v<sub>t</sub> / （1-\beta<sub>1</sub>）
	+ s<sub>t</sub> = s<sub>t</sub> / （1-\beta<sub>2</sub>）


----


## Nadam
> 类似于带有Nesterov动量项的Adam

![更新公式](https://i.loli.net/2019/04/28/5cc5838be4d71.jpg)

----














