
## 神经网络调参
#### 以下针对的是只含有全连接的网络
+ **参数：**
	+ 每层神经元个数
	+ 神经元层数
	+ 优化算法：
		+ SGD+momentum
		+ Adam
		+ Adadelta
		+ Adagrad：容易导致overfitting
	+ Batch size
	+ Learning rate
	+ Regularization
		+ 对权重进行L2/L1正则化
		+ Dropout
		+ Dropconnect
		+ Static Dropconnect

+ **方法：**
	+ 调整神经元层数，确保训练时的Loss下降；
	+ 找到一个overfitting配置，然后微调网络；
	+ 关键部分—优化方法：普通的SGD收敛速度较慢，但是训练好的模型通常会有更好的泛化效果。
	+ Batch size：批量过大容易overfitting，建议选择32或64左右的值，如果网络仍然overfitting，减少Batch_size。另外，Batch_size不易太小，否则梯度可能会有太多噪声。调整完Batch size后，应该去调整其它网络数量。
	+ 学习率：不能太高也不能太低。因此，最佳学习率取决于其它参数。通常从0.1开始，然后逐步减少它。有一条经验法则：将batch size增加alpha倍，也可以提高学习率alpha倍。
	+ 在数层之后将dropout作为第一层。
	+ static dropconnect：通常有一个密集连接的输入层，比如128个单位。将其改为一个非常巨大的隐藏层，比如4096个单位，这是一个巨大的网络，会严重overfitting。现在为了规范它，将这一层random dropout 99%，这是非常强的正则化，实践证明是可以的。













