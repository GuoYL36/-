
## 入门机器学习后的“随笔”

### 偏差和方差
+ 偏差：算法期望预测和真实预测之间的偏差程度。反应的是模型本身的拟合能力。
+ 方差：度量了同等大小的训练集的变动导致学习性能的变化，刻画了数据扰动导致的影响。
-----

### 欠拟合underfitting
+ **解决措施**
	+ 添加其它特征项（增大数据量）
	+ 添加多项式特征（增加网络层数）
	+ 减少正则化参数
	
### 过拟合overfitting
+ **解决措施**
	+ 重新清洗数据
	+ 增大数据的训练量
	+ 采用正则化方法
	+ 采用dropout法
	+ 提前终止训练
	+ 减少网络层数

### 梯度消失和梯度爆炸
+ **原理**
	+ 梯度消失：当网络层数很深时，如果梯度小于1，则会使得经过多层后向反馈后的梯度累乘远远小于1；
	+ 梯度爆炸：与梯度消失相反，当网络层数很深时，如果梯度大于1，则会使得经过多层后向反馈后的梯度累乘远远大于1；

+ **解决措施**
	+ 1. 激活函数的选择（使用ReLU代替sigmoid、tanh等）
	+ 2. 预训练 + 微调
	+ 3. 使用Batch Normalization
	+ 4. 使用残差网络结构
	+ 5. 使用LSTM网络
		+ 为什么LSTM比RNN更能解决梯度消失的问题？
			+ 因为在RNN中，BPTT的梯度是累乘形式，而RNN的输出中采用了tanh激活函数，所以会出现梯度消失问题；而LSTM的梯度除了累乘形式，还有累加形式，所以不容易出现梯度消失。
		+ LSTM中用sigmoid激活函数，而不用ReLU激活函数的原因？
			+ 因为在LSTM中，忘记门和更新门是起筛选作用，所以需要0~1之间的值作为概率来进行筛选。
	+ 6. 梯度剪切、权重正则

----
	
### 归一化
+ **机器学习为什么对数据进行归一化？**
	+ 归一化的目的：
		+ 处理不同规模和量纲的数据，使其缩放到相同的数据区间和范围，以减少规模、特征、分布差异对模型的影响。
		+ 归一化加速GD求解最优解的速度。比如收敛路径呈Z字型，导致收敛太慢；
		+ 归一化可能提高精度。
	
+ **机器学习什么情况下对数据进行归一化？**
	+ 使用了梯度下降算法，如LR、SVM等；
	+ 计算样本点距离时，如KNN、K-Means等。
	+ ......
+ **机器学习什么情况下不需要归一化？**
	+ 概率模型（决策树）不需要归一化。

+ **常用的归一化方法**
	+ max-min法：容易受极端值的影响，一定程度上会破坏原有的数据结构；
	+ z-score法：会改变原有数据的分布，不适合对稀疏数据做处理，不适合根据变量差异程度的聚类分析；
	+ RobustScaler：适用于存在离群点的数据。
	+ 上述方法分析：在分类中，聚类算法，数据符合正态分布中，需要使用距离来度量相似性或者使用PCA降维时，z-score表现得较好。在不涉及距离测量，协方差计算，数据不太符合正态分布时，可以使用第一种方法或其他方法。

+ **LR归一化问题，什么情况下可以不归一化，什么情况下必须归一化，为什么？**
	+ ......
-----

### Logistical Regression
+ **LR作为线性模型，如何拟合非线性情况？**
	+ **特征侧**：离散化、交叉组合
	+ **模型**：引入kernel，或推广到FM等model。

+ **提到LR损失函数要能知道交叉熵，为什么使用交叉熵？使用交叉熵为损失函数的优化问题是在优化什么量？交叉熵和KL散度、相对熵的关系？**
	+ ......
	+ ......

+ **LR的分布式实现逻辑是怎样的？数据并行和模型并行的区别？P-S架构是什么东西？**
	+ ......
	+ ......

-----
### 海量离散特征用简单模型、少量连续特征用复杂模型。
-----

### SVM
+ **优缺点**
	+ 解决小样本情况下的机器学习问题；
	+ 提高泛化能力；
	+ 处理高维空间数据；
	+ 解决非线性问题；
	+ 对于线性问题没有通用的解决方案，谨慎选择kernel函数；
	+ 处理分类问题时，要求解函数的二次规划问题，需要大量的存储空间。

+ **SVM中假设分离超平面系数非常大怎么办？**
	+ ......

+ **针对数据和特征的关系，如何选择SVM核函数？**
	+ 1. 特征数量和样本数量差不多时，选用LR或线性核的SVM；
	+ 2. 特征数量少、样本数量正常时，选用SVM+高斯核函数；
	+ 3. 特征数量少、样本数量很大时，需要手工添加一些特征变成第一种情况；
	+ 4. 特征数量少、样本数量少时，选用SVM + 非线性核。

+ **SVM常用核函数**
	+ linear核：主要用于线性可分情况，参数少，速度快；
	+ RBF核：主要用于线性不可分情况；（**注意：linear核是RBF核的一种特例**）
	+ sigmoid核

+ **常用核函数选择**
	+ 可以先用linear核函数看看效果，不好再用RBF核函数；
	+ 一般RBF核函数的性能位于linear核和sigmoid核之间，这也是为什么一般选用RBF核的原因。
	+ 如果对样本没有先验信息，则可以利用cross validation方法来评估不同核函数的性能，再选出性能最好的核函数。

-----

### LR和SVM的异同
+ **同：**
	+ 都是监督学习算法，都是判别模型；
	+ 都可以处理分类问题，一般都处理线性分类问题（**注意：LR也可以使用核函数**）；
	+ 两者都可以使用不同的正则化项，并在很多实验中，两者性能相当。
	
+ **异：**
	+ 损失函数不同，一个是Logistical loss，一个是Hinge loss；
	+ LR是参数模型，对异常值敏感；SVM是非参数模型；
	+ LR训练时考虑所有数据，容易受数据不平衡的影响；而SVM只依赖于支持向量，但是由于SVM基于间隔分类，一般要对数据做归一化处理。
	+ 对于非线性问题，SVM通常采用核函数解决，而LR很少用。这是因为使用核函数，SVM只要支持向量参与计算，而LR则是所有数据都参与计算，复杂度高。

----

### Random Forest
+ **为什么随机选取数据集？**
	+ 如果不随机的话，训练出来的多棵树的分类结果是一样的，违背了bagging思想;

+ **为什么有放回抽样？**
	+ RF在分类时是求同，有放回的抽样会产生相同的训练样本；如果不是有放回抽样，训练出来的每棵树的结果存在很大偏差，这样对分类结果没有任何帮助。所以有放回抽样能够减小bias，RF的目的是减小variance。

+ **影响RF分类结果的因素：**
	+ 任意两棵树的相关性：相关性越大，错误率越高；
	+ 每棵树的分类能力：分类器能力越强，错误率越低；
	+ 唯一可调参数：RF中特征子集的数量，数量越大，性能越好。

+ **RF的优缺点**
	+ RF引入两个随机性，抗噪能力增强；
	+ RF在分类中对各个变量的重要性进行估计，对泛化误差进行无偏估计；
	+ 可处理高维数据，对数据集的适应能力强；
	+ 性能由于单预测器，分类精度与boosting算法差不多，运行速度更快；
	+ 训练数据较少或噪声数据较大时，会发生overfitting。
-----

### GBDT
+ **GBDT优缺点：**
	+ 灵活处理各种离散值和缺失值；
	+ 对异常值具有鲁棒性；
	+ 相对SVM，预测的准确率较高；
	+ 由于弱学习器之间存在依赖关系，难以并行训练；
	+ 不适合处理高维稀疏数据，缺乏平滑处理。

+ **算法流程：**
	+ 多轮迭代，每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的**残差**基础上进行训练。对弱分类器的要求一般是低variance和高bias的简单分类器，如CART树。最终分类器是将每轮训练得到的弱分类器加权求和得到的。**（采用不放回采样）**

+ **损失函数：**
	+ 一般损失函数的负梯度(一阶导)作为残差；（因为一般损失函数存在优化困难问题，所以取一阶导）

+ **选择特征：**
	+ 以CART树为例，原始GBDT遍历每个特征，然后对每个特征遍历它所有可能的切分点，找到最优特征J和最优切分点，即 min(J){min(c1)sum(y-c1)^2+min(c2)sum(y-c2)^2}。

+ **分裂节点的评价指标：**
	+ 分裂时选择使得误差下降最多的分裂。

+ **构建特征：**
	+ 利用GBDT产生特征的组合(CTR预估中，工业届一般用LR处理非线性数据，其中就可以用这种特征组合的方式增强LR对非线性分布的拟合能力)。比如，利用GBDT生成两棵树，根据叶子结点的个数生成一个向量，然后将样本分别输入进去，样本分到叶子结点处该向量位置值为1，其余为0，根据得到的这个向量作为该样本的组合特征。

+ **用于分类：**
	+ GBDT无论用于分类还是回归都是用CART树。对于K分类问题，每次迭代构建K棵树，然后利用softmax求出属于每棵树的概率，残差是真实label与第k棵树的概率的差值。

+ **GBDT减少误差的方式：**
	+ 每棵树拟合当前模型预测值和真实值之间的误差。

+ **正则化：**
	+ **学习率**：越小越易overfitting；
	+ **子采样比**：越小越易underfitting
	+ **剪枝操作**

+ **参数**
	+ **步长、学习率**：减小步长，提高泛化能力；学习率过大容易overfitting；
	+ **树最大深度**：太大overfitting
	+ **内部节点再划分所需最小样本数和叶子节点最少样本数**：内部节点再划分所需最小样本数越大越防overfitting
	+ **子采样比**：太小容易underfitting

+ **相比于SVM、LR，GBDT的优势：**
	+ 基于树模型，继承了树模型的优点
		+ 对异常点鲁棒；
		+ 不相关特征干扰性低；
		+ 能处理缺失值；
		+ 受噪声干扰小。

+ **高维稀疏特征的时候，LR的效果为什么会比GBDT好？**
	+ 因为LR等线性模型的正则项是对权重进行惩罚，而树模型的惩罚项通常为叶子结点数量和深度。因此带正则化的线性模型不容易overfitting。

+ **为什么GBDT、XGBoost每棵树的深度很浅，而Decision Tree、RF每棵树的深度很深？**
	+ GBDT关注降低bias，每棵树浅带来的variance低；
	+ RF关注降低variance，每棵树深带来的bias低。

+ **GBDT和Adaboost的区别**
	+ 异
		+ GBDT是利用残差来训练每棵decision tree；Adaboost通过误差来调整每个样本的权重，从而训练每棵decision tree。
		+ 损失函数不同：GBDT使用一般损失函数；Adaboost使用指数损失函数。

	+ 同
		+ 都可以用来分类，相对于经典判别分类方法，效果好；
		+ 由多棵树组成。
		
### XGBoost
+ **为什么XGBoost要以二阶导数值作为权重进行分位？**
	+ 从目标函数可以看出，二阶导数值对 loss 有加权作用。
	
+ **XGBoost如何处理稀疏值？**
	+ 当特征出现稀疏值时，XGBoost可学习出默认的节点分裂方向。将缺失值向左、右节点各分一次，计算最大的Gain，向Gain最大的方向分。
	
+ **XGBoost的其它特性**
	+ 行抽样、列抽样（借鉴Random Foreast）
	+ Shrinkage，即学习率，减小学习率，迭代次数增加，有正则化作用。（减小学习率，相当于每个叶节点分数变小）
	+ 支持自定义损失函数（需二阶可导）。
----
	
+ **GBDT和XGBoost的区别**
	+ XGBoost支持线性分类器，传统GBDT以CART作为基分类器；XGBoost支持线性分类器，相当于带L1和L2正则化的LR或线性回归。
	+ GBDT对损失函数求一阶导，而XGBoost对损失函数进行二阶泰勒展开；XGBoost支持自定义代价函数，代价函数需一阶和二阶可导；
	+ XGBoost在代价函数里加入正则项，用于控制模型的复杂度；
	+ XGBoost支持列抽样，借鉴了RF的做法，能降低overfitting，减少计算；
	+ XGBoost工具支持并行，并行是在特征上做的，GBDT每次节点分裂时，需要对所有特征计算并排序，而XGBoost预先对数据排序并保存为block结构（预排序算法），迭代过程中重复使用这个结构，大大减少计算量。在节点分裂时，特征增益的计算可以开多线程实现。在内存有限或分布式的情况下，数据无法一次性载入时，XGBoost用可并行的近似直方图算法能高效生成候选的分割点。
-----

### Gradient Descent
+ **SGD和BGD的区别？**
	+ ...

+ **不同的GD方法有什么区别和联系？**
	+ ...

+ **二阶优化算法有什么？**
	+ ...

+ **对比off-line和on-line learning的区别？**
	+ ...
----

### L1、L2正则
+ **关于L1和L2正则，其几何解释和概率解释**
	+ ......

----

### Metrics

| 预测<br><br>实际| 1  | 0 |    |
| :--------: | :--------: | :--------: |:--------:|
| 1  |   TP   |   FN  |   TP+FN  |
| 0  |   FP   |   TN  |   FP+TN  |
|    | TP+FP  | FN+TN | TP+FN+FP+TN |

+ **准确率**：（TP+TN）/（TP+TN+FP+FN）
+ **错误率**：（FP+FN）/（TP+TN+FP+FN）
+ **召回率(Recall)、查全率**：实际的正样本中，分类成正样本的比例，（TP）/（TP+FN）
+ **精确率(Precision)、查准率**：分类成正样本中，实际的正样本的比例，（TP）/（TP+FP）
+ **真正例率（TPR）**：（TP）/（TP+FN），相当于Recall
+ **假正例率（FAR、FPR）**：（FP）/（FP+TN），也称误报率
+ **拒真率（FRR）**：实际的正样本中，分类成负样本的比例，（FN）/（TP+FN），相当于1-Recall
+ **F1分数**：精确率和召回率的调和指标，认为两个指标同等重要；
	+ 2×（precision×recall）/（precision+recall）
+ **F\_β分数**：（1+β×β）×（precision×recall）/（（β×β×precision）+recall）
+ **F2分数**：β等于2，召回率的重要程度是精确率的2倍
+ **G分数**：精确率和召回率的几何平均，sqrt(precision×recall)
----

+ **ROC曲线**
> 不平衡数据集中最常用的指标之一

![ROC曲线](https://s2.ax1x.com/2019/04/06/AWLVLF.jpg "ROC曲线")

+ **几个结论：**
	+ 阈值变大，样本分类成负样本的可能性增大（FP ↓，FN ↑）；阈值变小，样本分类成正样本的可能性增大（TP、FP ↑）；
	+ 好的分类模型应该尽可能位于坐标轴的左上方；随机猜测模型应该靠近于（0,0）和（1,1）这条对角线上；
	+ AUC面积值位于[0,1]，值越大说明模型分类越好；
+ **针对不平衡数据，应该如何选择阈值？**
	+ 相等错误率（ERR），FAR随阈值的增大而减少，FRR随阈值的增大而增大。当FAR=FRR时，此时阈值最合适。
----

